

## 经典算法
### 线性回归
- 最小二乘估计和极大似然的关系？
 1. 假定误差服从高斯分布，认为样本是独立，使用最大似然估计就能得出的结论

- 为什么用梯度下降？
 1. 不是满秩矩阵,则就不存在逆矩阵.或者当特征维度非常大时求逆运算时间开销很大，用梯度下降法对最小二乘法形式的误差损失函数进行优化


### 逻辑回归、评分卡
- iv woe psi的计算，woe转化的好处（解释性，减少异常点的影响，加快收敛）, 每个区间woe和坏账率的关系，概率到分数如何建立映射？
- 评分卡模型系数有什么要求？显著性？方差膨胀因子 VIF：Variance inflation factor，一般大于5或10的变量删除？ 
- 拟合度曲线 
- 损失函数（对数似然求平均的相反数），公式推导出梯度。 
- 最后的结果梯度 = 误差 * 特征 f9922
- softmax是如何计算的？ 
- 为何用对数损失而不是平方损失？梯度会消失、平方损失非凸求不到最优解
- 当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪 
些场景，它们之间又有怎样的关系？ 
- 为什么用sigmoid函数？一是： sigmod 本身的性质。 二是：之所以LR 用sigmod，不是因为LR 选择了 sigmod ，而是用 指数簇分布和 最大熵原理 推导出来的形式，就是这个样子，后来起名叫sigmod

### 决策树
- 最优特征选择的准则？
- 剪枝方法（预剪枝、后剪枝）代价复杂度剪枝
- ID3, C4.5, CART(分类和回归的区别)的原理和优缺点？
从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以
处理连续型变量。

这三种决策树还有一些不同。比如，
ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处
理；ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复
用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特
征可以被重复使用；ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART
直接利用全部数据发现所有可能的树结构进行对比。

### 模型融合（集成学习）
- boosting和bagging的思想，联系和区别？如何从减小方差和偏差的角度解释Boosting和Bagging的原理

- adaboost分类的损失函数？AdaBoost 算法是前向分步算法的特例。此时，基函数为基分类器，损失函数为指数函数L(y,f(x)) = exp(-y*f(x))
- 对于二分类问题，提升树算法只需要将AdaBoost 算法中的基学习器限制为二叉分类树即可
- 讲讲提升树和梯度提升树的原理？计算梯度的自变量是什么，损失函数一般是什么？如何控制GBDT过拟合？【通过验证集来确定树的棵数M的大小（正则化）；叶子节点中包含的样本最少个数；加入决策树的惩罚项用于后剪枝（结构风险）（每棵树叶子节点个数在4—8之间或者关于叶结点预测值的平方和的L2惩罚项）
梯度提升迭代次数M；通过缩减学习率v；随机梯度提升（每个基学习器的采样数为0.5），减少了方差（过拟合）；】
- 梯度提升的优点
（1）预测阶段的计算速度快，树与树之间可并行化计算。
（2）在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在
Kaggle的众多竞赛中，经常名列榜首。
（3）采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，
能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一
化等。
■ 局限性
（1）GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
（2）GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理
数值特征时明显。
（3）训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提
高训练速度。

- xgb的损失函数写到二阶，推理到按照叶子节点对损失求和。节点分数
- xgboost特征评分的原理？【在训练的过程中，通过结构分数的增益情况选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。】
- Xgboost的正则化包括？
- 缺失值如何处理？
- xgb后剪枝的过程（XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值（得分），它会去除这个分裂。）
- xgb如何获得最优boosting迭代次数？
- 分裂点寻找近似算法？并行计算？
- 传统 CART树寻找最优切分点的标准是最小化均方差；
XGBoost 通过最大化得分公式来寻找最优切分点：
优化：XGBoost 实现了一种近似的算法，大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。
XGBoost 考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper 提到能提高 50 倍。
特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然 Boosting 算法迭代必须串行，但是在处理每个特征列时可以做到并行。
按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致 cache miss，降低算法效率。Paper 中提到，可先将数据收集到线程内部的 buffer，然后再计算，提高算法的效率。
XGBoost 还考虑了数据量比较大的情况，当内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。
为什么不稳定的学习器更适合作为基学习器？

- 梯度提升和梯度下降的区别和联系是什么

- LightGBM优点：更快的训练速度
    更低的内存消耗
    更好的准确率
    分布式支持，可以快速处理海量数据
 优化：   
    基于 Histogram 的决策树算法
    带深度限制的 Leaf-wise 的叶子生长策略
    直接支持类别特征(Categorical Feature)
    直接支持高效并行：支持特征并行和数据并行的两种

    直方图做差加速
    Cache 命中率优化
    基于直方图的稀疏特征优化
    多线程优化。

- pasting和bagging采样的区别？
- rf随机性体现？随机森林重要性的计算？可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？
- bagging之神经网络的 Dropout 策略
- 不稳定的学习器容易受到样本分布的影响（方差大），很好的引入了随机性；这有助于在集成学习（特别是采用 Bagging 策略）中提升模型的泛化能力。
为了更好的引入随机性，有时会随机选择一个属性子集中的最优分裂属性，而不是全局最优（随机森林）

神经网络也属于不稳定的学习器；
此外，通过调整神经元的数量、网络层数，连接方式初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。
Bagging 方法中能使用线性分类器作为基学习器吗？ Boosting 呢？

Bagging 方法中不推荐
线性分类器都属于稳定的学习器（方差小），对数据不敏感；
甚至可能因为 Bagging 的采样，导致在训练中难以收敛，增大集成分类器的偏差
Boosting 方法中可以使用
Boosting 方法主要通过降低偏差的方式来提升模型的性能，而线性分类器本身具有方差小的特点，所以两者有一定相性
XGBoost 中就支持以线性分类器作为基学习器。
- lr + 树模型
- stacking or blending原理？

### 模型对比：

#### GBDT与Logistic Regression的区别总结？
1. 从机器学习三要素的角度：
1.1 模型
本质上来说，他们都是监督学习，判别模型，直接对数据的分布建模，不尝试挖据隐含变量，这些方面是大体相同的。但是又因为一个是线性模型，一个是非线性模型，因此其具体模型的结构导致了VC维的不同：
其中，Logistic Regression作为线性分类器，它的VC维是d+1，而 GBDT 作为boosting模型，可以无限分裂，具有无限逼近样本VC维的特点，因此其VC维远远大于d+1，这都是由于其线性分类器的特征决定的，归结起来，是Logistic Regression对数据线性可分的假设导致的。

1.2 策略
从 Loss(经验风险最小化) + 正则(结构风险最小化) 的框架开始说起；

从Loss的角度：
因为 Logistic Regression 的输出是 y = 1 的概率，所以在极大似然下，Logistic Regression的Loss是交叉熵，此时，Logistic Regression的准则是最大熵原理，也就是“为了追求最小分类误差，追求最大熵Loss”，本质上是分类器算法，而且对数据的噪声具有高斯假设；
而 GBDT 采用 CART 树作为基分类器，其无论是处理分类还是回归均是将采用回归拟合（将分类问题通过 softmax 转换为回归问题），用当前轮 CART 树拟合前一轮目标函数与实际值的负梯度，本质上是回归算法。

也正是因为 GBDT 采用的 CART 树模型作为基分类器进行负梯度拟合，其是一种对特征样本空间进行划分的策略，不能使用 SGD 等梯度优化算法，而是 CART 树自身的节点分裂策略：均方差(回归) 也带来了算法上的不同；
GBDT 损失函数值得是前一轮拟合模型与实际值的差异，而树节点内部分裂的特征选择则是固定为 CART 的均方差，目标损失函数可以自定义，当前轮 CART 树旨在拟合负梯度。

从特征空间的角度:
就是因为 Logistic Regression 是特征的线性组合求交叉熵的最小化，也就是对特征的线性组合做 logistic，使得Logistic Regression会在特征空间中做线性分界面，适用于分类任务；
而 GBDT 采用 CART 树作为基分类器，其每轮树的特征拟合都是对特征空间做平行于坐标轴的空间分割，所以自带特征选择和可解释性，GBDT 即可处理分类问题也可解决回归问题，只是其统一采用回归思路进行求解（试想，如果不将分类转换为回归问题，GBDT 每轮目标函数旨在拟合上一轮组合模型的负梯度，分类信息无法求梯度，故而依旧是采用 softmax 转换为回归问题进行求解）。

从正则的角度：

Logistic Regression 的正则采用一种约束参数稀疏的方式，其中 L2 正则整体约束权重系数的均方和，使得权重分布更均匀，而 L1 正则则是约束权重系数绝对值和，其自带特征选择特性；

GBDT 的正则：

弱算法的个数T，就是迭代T轮。T的大小就影响着算法的复杂度
步长（Shrinkage）
最小分裂增益(sklearn)
max_depth(sklearn)
xgboost还加入叶子节点数，叶子权值平方和的显示正则项

1.3 算法
Logistic Regression 若采用 SGB, Momentum, SGD with Nesterov Acceleration 等算法，只用到了一阶导数信息，若用 AdaGrad, AdaDelta / RMSProp, Adam, Nadam, 牛顿法则用到了二阶导数信息，
而GBDT 直接拟合上一轮组合函数的特梯度，只用到了一阶倒数信息，XGBoost 则是用到了二阶导数信息。


2. 从特征的角度：

2.1 特征组合：

如前所说，GBDT 特征选择方法采用最小化均方损失来寻找分裂特征及对应分裂点，所以自动会在当前根据特征 A 分裂的子树下寻求其他能使负梯度最小的其他特征 B，这样就自动具备寻求好的特征组合的性能，因此也能给出哪些特征比较重要（根据该特征被选作分裂特征的次数）
而 LR 只是一次性地寻求最大化熵的过程，对每一维的特征都假设独立，因此只具备对已有特征空间进行分割的能力，更不会对特征空间进行升维（特征组合）


2.2 特征的稀疏性：

如前所述，Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，因此只具有线性分界面，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。
而对于 GBDT，其更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低纬稠密特征，也进一步引入了语意信息，有利于特征的表达。

3. 数据假设不同：

逻辑回归对数据有两个假设：第一个基本假设是假设数据服从伯努利分布。

第二个假设是假设样本为正的概率是 ：

gbdt对数据没有假设


#### LR和GBDT高维稀疏特征？

面试被问到GBDT是否适合处理高维稀疏特征，没有答上来，感觉自己对模型理解深度不够。
结论：LR适合处理高维稀疏特征，而GBDT不适合。
主要原因有：
1、耗时：高维特征会导致gbdt运行过于耗时
2、过拟合：从高维稀疏特征中难以进行有效的特征空间划分，且对噪音会很敏感。
想想一个例子，有个年龄特征0~100，如果对这样特征进行one-hot编码后变为稀疏特征，第i维表示是否为i岁。
如果将这种特征直接输入gbdt然后输出是否是青年人。很显然gbdt将变成枚举各个年龄是否为青年人。这类特征是非常容易过拟合的，如果当训练样本中存在一些噪声样本如80岁的青年人，如果在80岁没有足够的样本，这个错误将被gbdt学到。而如果直接采用连续特征进行分类，gbdt会有更好的泛化性能。
3、高维稀疏特征大部分特征为0，假设训练集各个样本70%的特征为0，30%的特征非0。则某个维度特征在所有样本上也期望具有近似的取0的比例。当作分裂时，特征选择非常低效，特征只会在少部分特征取值非0的样本上得到有效信息。而稠密向量可以得到样本集的整体特征信息。
至于LR为什么在高维稀疏特征上表现较好。我的理解是：
1、不会过拟合：LR的目标就是找到一个超平面对样本是的正负样本位于两侧，由于这个模型够简单，不会出现gbdt上过拟合的问题。
2、不会欠拟合：高维稀疏特征是不是可以理解为低维的稠密特征映射到了高维空间。这里联想到了SVM的核技巧，不也是为了将特征由低维空间映射到高维空间中实现特征的线性可分吗？在SVM中已经证实了其有效性。这里面应该存在某种规律，LR在高维空间比低维空间中具有更高的期望实现更好分类效果的。


### 关联规则
- 挖掘的3个度量指标：支持度、置信度、提升度
- 判断规则的有效性

### 概率图模型

### 非监督模型

### 降维

