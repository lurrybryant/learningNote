- 循环神经网络RNN：
 见自然语言处理
Simple RNNs(SRNs)
Bidirectional RNNs
Deep RNNs
Echo State Networks（ESNs）
Gated Recurrent Unit Recurrent Neural Networks
Bidirectional LSTMs
Stacked LSTMs
Clockwork RNNs(CW-RNNs)
CNN-LSTMs

Soft Attention 与Hard Attention

处理文本数据时，循环神经网络与前馈神经网络相比有什么特点?
传统文本处理任务的方法中一般将TF-IDF向量作为特征输入，丢失了输入的文本序列中每个单词的顺序，卷积神经网络对文本数据建模时，输入变长的字符串或者单词串，然后通过滑动窗口加池化的方式将原先的输入转换成一个固定长度的向量表示，这样做可以捕捉到原文本中的一些局部特征，但是两个单词之间的长距离依 赖关系还是很难被学习到。循环神经网络却能很好地处理文本数据变长并且有序的输入序列。它模拟了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，将前面阅读到的有 用信息编码到状态变量中去，从而拥有了一定的记忆能力，可以更好地理解之后 的文本。下一时刻状态转移的权重矩阵。在文本分类任务中，f可以选取Tanh函数或者ReLU函数，g可以采用Softmax函数。相比于卷积神经网络等前馈神经网络，循环神经网络由于具备对序列顺序信 息的刻画能力，往往能得到更准确的结果。


循环神经网络为什么会出现梯度消失或梯度爆炸?有哪些改进方案?
梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值时，对梯度进行等比收缩。而梯度消失问题相对比较棘手，需要对模型本身进行 改进。深度残差网络是对前馈神经网络的改进，通过残差学习的方式缓解了梯度消失的现象，从而使得我们能够学习到更深层的网络表示;而对于循环神经网络来说，长短时记忆模型[23]及其变种门控循环单元(Gated recurrent unit，GRU)[24] 等模型通过加入门控机制，很大程度上弥补了梯度消失所带来的损失。


在循环神经网络中能否使用ReLU作为激活函数?
但是需要对矩阵的初值做一定限制，否则十分容易引发数值问题。为什么在卷积神经网络中不会出现这样的现象呢?这是因为在卷积神经网络中每一层的权重矩阵W是不同的，并且在初始化时它们是独立同分布的，因此可以相互 抵消，在多层之后一般不会出现严重的数值问题。当采用ReLU作为循环神经网络中隐含层的激活函数时，只有当W的取值在单位矩阵附近时才能取得比较好的效果，因此需要将W初始化为单位矩阵。实验证明，初始化W为单位矩阵并使用ReLU激活函数在一些应用中取得了与长短期记忆模型相似的结果，并且学习速度比长短期记忆模型更快，是一个值得尝试的小技巧[25]。


LSTM是如何实现长短期记忆功能的?
加入了输入门it、遗忘门ft以及输出门ot三个门和 一个内部记忆单元ct。输入门控制当前计算的新状态以多大程度更新到记忆单元中;遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉;输出门控制当前 的输出有多大程度上取决于当前的记忆单元。


LSTM里各模块分别使用什么激活函数，可以使用别的激活函数吗?
遗忘门、输入门和输出门使用Sigmoid函数作为激活函数;在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。值 得注意的是，这两个激活函数都是饱和的，也就是说在输入达到一定值的情况 下，输出就不会发生明显变化了。如果是用非饱和的激活函数，例如ReLU，那么 将难以实现门控的效果。 在门控中，使用Sigmoid函数是几乎所有现代神经网络 模块的共同选择。例如在门控循环单元和注意力机制中，也广泛使用Sigmoid函数 作为门控的激活函数。


什么是Seq2Seq模型?Seq2Seq模型有哪些优点?
这一过程由编码输入与解码输出两个环节构成。在经典的 实现中，编码器和解码器各由一个循环神经网络构成，既可以选择传统循环神经 网络结构，也可以使用长短期记忆模型、门控循环单元等。在Seq2Seq模型中，两 个循环神经网络是共同训练的。


Seq2Seq模型在解码时，有哪些常用的办法?
Seq2Seq模型最基础的解码方法是贪心法，即选取一种度量标准后，每次都在 当前状态下选择最佳的一个结果，直到结束。
集束搜索是常见的改进算法，它是一种启发式算法。该方法会保存beam size(后面简写为b)个当前的较佳选择，然后解码时每一步根据保存的选择进行 下一步扩展和排序，接着选择前b个进行保存，循环迭代，直到结束时选择最佳的 一个作为解码的结果。
解码时使用堆叠的RNN、增加Dropout机制、与编码器之间建立残差连接等， 均是常见的改进措施。
解码环节中一个重要的改进是注意力机制在解码时每一步可以有针对性地关注与当前有关的 编码结果，从而减小编码器输出表示的学习难度，也更容易学到长期的依赖关系 。此外，解码时还可以采用记忆网络[28]等，从外界获取知识。


Seq2Seq模型引入注意力机制是为了解决什么问题?为什么选用了双向的循环 神经网络模型?



# 面试题目
处理文本数据时，循环神经网络与前馈神经网络相比有什么特点?
循环神经网络为什么会出现梯度消失或梯度爆炸?有哪些改进方案?
在循环神经网络中能否使用ReLU作为激活函数?
LSTM是如何实现长短期记忆功能的?
LSTM里各模块分别使用什么激活函数，可以使用别的激活函数吗?
什么是Seq2Seq模型?Seq2Seq模型有哪些优点?
Seq2Seq模型在解码时，有哪些常用的办法?
Seq2Seq模型引入注意力机制是为了解决什么问题?为什么选用了双向的循环神经网络模型?


# 论文



