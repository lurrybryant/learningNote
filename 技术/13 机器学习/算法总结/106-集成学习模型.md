## 集成学习
ensemble模型：http://blog.csdn.net/dengxing1234/article/details/73739836
boosting（提升树，GBDT，XGBoost，Adaboost，catboost，lightGBM），
bagging（RF，ET，隔离森林IsolationForest，
    完全随机树嵌入，投票，
    Mondrian Forests: Bernoulli Random Forests
    TensorForest: ggRandomForests:
    ParallelForest: gcforest）
stacking，
blending


好处
  统计角度：多个学习器的结合可以减少泛化能力差的风险
  优化角度：降低陷入糟糕局部极小点的风险
  表示角度：扩大了假设空间，有可能学得更好的近似

步骤
  产生个体学习器；
  按照某种策略结合；
  集成修剪；

按照个体学习器是否相同
  同质：个体学习器相同，称为基学习器；
  异质：个体学习器不同，称为组件学习器；

如果基学习器的误差相互独立，则集成后的错误率将指数下降。因此要求个体学习器准确、多样
  数据样本扰动对不稳定学习器尤其有效
  输入属性扰动，随机子空间算法
  输出表示扰动
  算法参数扰动

结合策略
  平均法（简单平均、加权平均、贝叶斯模型平均BMA）
  投票法（绝对多数、相对多数、加权，硬投票、软投票）

解释性差

boosting和bagging的思想，联系和区别？
  Boosting方法：串行，有依赖，基本思路（是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重，测试时根据各层分类器的结果的加权得到最终结果），降低偏差（线性分类器本身具有方差小的特点，所以两者有一定相补性，XGBoost中就支持以线性分类器（支持lr + 树模型）作为基学习器）；
  Bagging方法：并行，无强依赖，基本思想（采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合），减小方差；

最常用的基分类器是决策树，主要有以下3个方面的原因：
(1)决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。
(2)决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。
(3)数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。

除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型也比较“不稳定”，而且还可以通过调整神经元数量、连接方式、网络层数、初始权值等方式引入随机性。

如何从减小方差和偏差的角度解释Boosting和Bagging的原理?
Bagging对n个独立不相关的模型的预测结果取平均，方差是原来单个模型的1/n，
boosting在训练好一个弱分类器 后，我们需要计算弱分类器的错误或者残差，作为下一个分类器的输入。这个过 程本身就是在不断减小损失函数，来使模型不断逼近“靶心”，使得模型偏差不断降低。各弱分类器之间是强相关的，缺乏独立性，所以并不会对降低方差有作用。


## Stacking
原理
  k折交叉
  每对训练集/验证集，T个分类器逐个训练和预测
  新的特征包含T个维度，再交叉训练出一个次级模型

典型：StackingC in Weka.  mlxtend
  初级学习器的输出类概率作为次级学习器的输入属性
  多响应线性回归（MLR）作为次级学习器
  贝叶斯模型平均（BMA）


## Blending
原理


典型：


## bagging 和 rf
特点
  弱依赖
  并行
  关注方差
  样本扰动

策略
  结合策略：
  平均法；投票法；学习法stacking
  强可学习和弱可学习

算法
  学习器弱依赖关系：并行，比如随机森林；
  关注方差；
  个体学习器要准确多样；
  样本扰动 + 属性扰动 = 多样性

特点
  样本扰动和属性扰动增加多样性；
  简单容易实现、计算开销小、效率高
  自助采样包外样本
  Feature projection\Feature expansion\branch random
  降低方差，对易受样本扰动的学习器上效用更为明显比如：决策树、神经网络；

随机森林如何评估特征重要性（http://charleshm.github.io/2016/03/Random-Forest-Tricks/）?随机森林特征重要性：http://blog.csdn.net/m0_37770941/article/details/78330795
  - 计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性
  - Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。
  - Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。
  - 包外分数：http://blog.csdn.net/edogawachia/article/details/79357844

简述Bagging算法过程？
  随机森林：属性选择扰动；样本扰动（自助采样，有放回），提高基学习器之间的差异性进而提高泛化性能；
  数据样本扰动，输入属性扰动，输出表示扰动，算法参数扰动；

可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻?
  线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大，所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，而导致他们在训练中更难收敛，从而增大了集成分类器的偏差。

Pasting：无放回采样
  Bagging和Pasting都允许在多个分类器上对训练集进行多次采样，但只有Bagging允许在同一种分类器上对训练集进行进行多次采样
  随机贴片：对特征和实例采样
  随机空间：只对特征采样

为什么不稳定的学习器更适合作为基学习器？  
  - 不稳定的学习器容易受到样本分布的影响（方差大），很好的引入了随机性，这有助于在集成学习（特别是采用 Bagging 策略）中提升模型的泛化能力。
  - 为了更好的引入随机性，有时会随机选择一个属性子集中的最优分裂属性，而不是全局最优（随机森林）；
  - 神经网络也属于不稳定的学习器，通过调整神经元的数量、网络层数、连接方式、初始权重也能很好的引入随机性和改变模型的表达能力和泛化能力。bagging之神经网络的Dropout策略；

Bagging方法中能使用线性分类器作为基学习器吗？ Boosting呢？
  Bagging方法中不推荐，线性分类器都属于稳定的学习器（方差小），对数据不敏感；甚至可能因为Bagging的采样，导致在训练中难以收敛，增大集成分类器的偏差


## 极端随机树
特征切分点的选择随机化


## Adaboost
损失函数为指数函数，学习算法为前向分步算法；
  学习期强依赖关系：串行Boosting；比如GBDT
  重新赋权值，重新采样；
  通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能
  加大误分类样本和误差率小的弱分类器的权重；
  关注偏差；  
  指数损失；
  模型为加法模型，策略是指数损失函数（训练误差以指数速率下降），算法为前向分步算法（训练数据初始权重为1/n，训练出一个分类器并得到误差率和该分类器的权重，接着更新训练数据的权值分布或者重采样）由前向分步算法思想，将基本分类器设为决策树（二叉分类树和二叉回归树），回归问题损失函数为平方误差损失，可以转化为拟合残差或损失函数的负梯度在当前模型的值（GDBT），分类问题为指数损失。损失函数为指数函数L(y,f(x)) = exp(-y*f(x))


## 提升树和GBDT
核心思想
  提升：每一步产生一个弱预测模型(如决策树)，并加权累加到总模型中。在任意时间t，根据t-1时刻得到的结果我们给当前结果赋予一个权重。之前正确预测的结果获得较小权重，错误分类的结果得到较大权重。
  梯度提升(Gradient boosting)：弱预测模型生成依据损失函数的梯度方向
  梯度提升算法：首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合(基函数)，即自变量就是每次加进来的基函数；通过迭代地选择一个负梯度方向上的基函数来逐渐逼近局部极小值。
  提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器
  学习强依赖关系：串行Boosting；关注偏差，比如GBDT、提升树
  提升树：通过经验风险最小化确定下一棵决策树。二分类问题的基分类器为二分类树的提升方法是ADBoost的特例。

GBDT:
  利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似，拟合一个回归树；该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。    
  根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。算法1描述了GradientBoosting算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。


模型
  树的线性组合

策略
  分类问题为指数损失，回归问题为平方损失等

算法
  回归问题提升树拟合当前残差：确定损失函数，前向分步算法，拟合当前残差，当损失函数是其它损失函数时，前向分步的加法模型就不好算了；
  GBDT拟合损失函数在当前模型的负梯度；

模型特点应用场景
优点：
  低泛化误差；
  没有太多参数可以调；
缺点：
  对离群点比较敏感；

Gradientboosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？
答：最根本的区别是，随机森林算法使用bagging技术做出预测；而GBM是采用boosting技术做预测的。
  1.在bagging技术中，数据集用随机采样的方法被划分成n个样本。然后，使用单一的学习算法，在所有样本上建模。接着利用投票或者求平均来组合所得到的预测。bagging是平行进行的，随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。
  2.而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在后续一轮中得到校正。这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。GBM提高了精度，同时减少了模型的偏差和方差


GBDT为什么对损失函数求负梯度可以达到拟合残差的作用？
  泰勒一阶展开解释。梯度提升法。

如何控制GBDT过拟合？
通过验证集来确定树的棵数M的大小（正则化）；
叶子节点中包含的样本最少个数；
加入决策树的惩罚项用于后剪枝（结构风险）（每棵树叶子节点个数在4—8之间或者关于叶结点预测值的平方和的L2惩罚项）
梯度提升迭代次数M；
通过缩减学习率v；
随机梯度提升（每个基学习器的采样数为0.5），减少了方差（过拟合）；

梯度提升
  优点
    预测阶段的计算速度快，树与树之间可并行化计算。
    在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
    采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且也不需要对数据进行特殊的预处理如归一化等。
  局限性
    GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
    GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
    训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。

梯度提升和梯度下降的区别和联系是什么?
    两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新；只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参数的更新，而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。

维基百科：https://en.wikipedia.org/wiki/Gradient_boosting#Shrinkage
  Blog: https://blog.csdn.net/Mr_tyting/article/details/76416176
  https://blog.csdn.net/han_xiaoyang/article/details/52665396
  https://blog.csdn.net/han_xiaoyang/article/details/52663170
  https://blog.csdn.net/niaolianjiulin/article/details/76584785
  安装http://lightgbm.apachecn.org/cn/latest/Installation-Guide.html
  https://blog.csdn.net/weiyongle1996/article/details/78446244


## XGBoost

基本原理：
  Boosting Tree构造树来拟合残差，而Xgboost引入了二阶导来进行求解；并且引入了节点的数目、参数的L2正则来评估模型的复杂度；构造Xgboost的预测函数与目标函数；在分裂点选择的时候也以目标函数最小化为目标。

特点：
  XGBoost的实现中使用了并行/多核计算，因此训练速度快；同时它的原生语言为C/C++，这是它速度快的实践原因，相对于传统的GBDT，XGBoost使用了二阶信息，可以更快的在训练集上收敛；
  XGBoost以“正则化提升(regularized boosting)”技术而闻名；
  XGBoost允许用户定义自定义优化目标和评价标准；
  XGBoost内置处理缺失值的规则；         
  XGBoost会一直分裂到指定的最大深度max_depth，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂；
  XGBoost允许在每一轮boosting迭代中使用交叉验证，因此可以方便地获得最优boosting迭代次数；
  实现了分裂点寻找近似算法：传统CART树寻找最优切分点的标准是最小化均方差；XGBoost通过最大化得分公式来寻找最优切分点：XGBoost 实现了一种近似的算法，大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。Paper中提到，可先将数据收集到线程内部的 buffer，然后再计算，提高算法的效率。
XGBoost 还考虑了数据量比较大的情况，当内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。
  利用了特征的稀疏性，XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper 提到能提高 50 倍；
  数据事先排序并且以block形式存储，有利于并行计算？特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然 Boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。
  基于分布式通信框架rabit，可以运行在MPI和yarn上。（最新已经不基于rabit了） 
  做了面向体系结构的优化，针对cache和内存做了性能优化；
  xgb的损失函数写到二阶，推理到按照叶子节点对损失求和；

XGBoost与GBDT的联系和区别有哪些?
  损失函数：GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
  基分类器：传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。
  随机性：传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
  缺失值：传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。
  过拟合：由于损失函数前后存在差值一定为正的限制，此时γ起到了一定的预剪枝效果；XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；
  实现：GBDT是机器学习算法，XGBoost是该算法的工程实现。

xgboost调参：https://blog.csdn.net/q383700092/article/details/53763328
安装xgboost https://www.cnblogs.com/haobang008/p/5907854.html

xgboost怎么给特征评分？
  在训练的过程中，通过结构分数的增益情况选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。



## Lightgbm
特点：
  更快的训练效率
  低内存使用
  更高的准确率
  支持并行化学习
  可处理大规模数据
  基于Histogram的决策树算法
  带深度限制的Leaf-wise的叶子生长策略
  直方图做差加速
  直接支持类别特征(Categorical Feature)
  Cache命中率优化
  基于直方图的稀疏特征优化
  多线程优化


## 模型对比

GBDT与Logistic Regression的区别总结？
   - 从机器学习三要素的角度：
    模型
本质上来说，他们都是监督学习，判别模型，直接对数据的分布建模，不尝试挖据隐含变量，这些方面是大体相同的。但是又因为一个是线性模型，一个是非线性模型，因此其具体模型的结构导致了VC维的不同：
  其中，Logistic Regression作为线性分类器，它的VC维是d+1，而 GBDT 作为boosting模型，可以无限分裂，具有无限逼近样本VC维的特点，因此其VC维远远大于d+1，这都是由于其线性分类器的特征决定的，归结起来，是Logistic Regression对数据线性可分的假设导致的。
    策略
  从 Loss(经验风险最小化) + 正则(结构风险最小化) 的框架开始说起；
  从Loss的角度：因为 Logistic Regression 的输出是 y = 1 的概率，所以在极大似然下，Logistic Regression的Loss是交叉熵，此时，Logistic Regression的准则是最大熵原理，也就是“为了追求最小分类误差，追求最大熵Loss”，本质上是分类器算法，而且对数据的噪声具有高斯假设；而 GBDT 采用 CART 树作为基分类器，其无论是处理分类还是回归均是将采用回归拟合（将分类问题通过softmax转换为回归问题），用当前轮 CART 树拟合前一轮目标函数与实际值的负梯度，本质上是回归算法。也正是因为GBDT采用的CART树模型作为基分类器进行负梯度拟合，其是一种对特征样本空间进行划分的策略，不能使用SGD等梯度优化算法，而是CART树自身的节点分裂策略：均方差(回归) 也带来了算法上的不同；GBDT损失函数值得是前一轮拟合模型与实际值的差异，而树节点内部分裂的特征选择则是固定为CART的均方差，目标损失函数可以自定义，当前轮CART树旨在拟合负梯度。
  从特征空间的角度:就是因为 Logistic Regression 是特征的线性组合求交叉熵的最小化，也就是对特征的线性组合做 logistic，使得Logistic Regression会在特征空间中做线性分界面，适用于分类任务；而 GBDT 采用 CART 树作为基分类器，其每轮树的特征拟合都是对特征空间做平行于坐标轴的空间分割，所以自带特征选择和可解释性，GBDT 即可处理分类问题也可解决回归问题，只是其统一采用回归思路进行求解；
  从正则的角度：Logistic Regression 的正则采用一种约束参数稀疏的方式，其中 L2 正则整体约束权重系数的均方和，使得权重分布更均匀，而L1正则则是约束权重系数绝对值和，其自带特征选择特性；GBDT的正则：弱算法的个数T，步长（Shrinkage），最小分裂增益(sklearn)，max_depth(sklearn)，xgboost还加入叶子节点数，叶子权值平方和的显示正则项；
    算法
Logistic Regression 若采用 SGB, Momentum, SGD with Nesterov Acceleration 等算法，只用到了一阶导数信息，若用 AdaGrad, AdaDelta / RMSProp, Adam, Nadam, 牛顿法则用到了二阶导数信息，
而GBDT 直接拟合上一轮组合函数的负梯度，只用到了一阶倒数信息，XGBoost 则是用到了二阶导数信息。
   
   - 从特征的角度：
   特征组合：
如前所说，GBDT 特征选择方法采用最小化均方损失来寻找分裂特征及对应分裂点，所以自动会在当前根据特征 A 分裂的子树下寻求其他能使负梯度最小的其他特征B，这样就自动具备寻求好的特征组合的性能，因此也能给出哪些特征比较重要（根据该特征被选作分裂特征的次数）而 LR 只是一次性地寻求最大化熵的过程，对每一维的特征都假设独立，因此只具备对已有特征空间进行分割的能力，更不会对特征空间进行升维（特征组合）

   特征的稀疏性：
如前所述，Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，因此只具有线性分界面，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。而对于 GBDT，其更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低维稠密特征，也进一步引入了语意信息，有利于特征的表达。

   数据假设不同：
逻辑回归对数据有两个假设：第一个基本假设是假设数据服从伯努利分布；第二个假设是假设样本为正的概率是 ：；gbdt对数据没有假设
  LR和GBDT高维稀疏特征：LR适合处理高维稀疏特征，而GBDT不适合。
1、耗时：高维特征会导致gbdt运行过于耗时
2、过拟合：从高维稀疏特征中难以进行有效的特征空间划分，且对噪音会很敏感。想想一个例子，有个年龄特征0~100，如果对这样特征进行one-hot编码后变为稀疏特征，第i维表示是否为i岁。如果将这种特征直接输入gbdt然后输出是否是青年人。很显然gbdt将变成枚举各个年龄是否为青年人。这类特征是非常容易过拟合的，如果当训练样本中存在一些噪声样本如80岁的青年人，如果在80岁没有足够的样本，这个错误将被gbdt学到。而如果直接采用连续特征进行分类，gbdt会有更好的泛化性能。
3、高维稀疏特征大部分特征为0，假设训练集各个样本70%的特征为0，30%的特征非0。则某个维度特征在所有样本上也期望具有近似的取0的比例。当作分裂时，特征选择非常低效，特征只会在少部分特征取值非0的样本上得到有效信息。而稠密向量可以得到样本集的整体特征信息。

  至于LR在高维稀疏特征上表现较好：
1、不会过拟合：LR的目标就是找到一个超平面对样本是的正负样本位于两侧，由于这个模型够简单，不会出现gbdt上过拟合的问题。
2、不会欠拟合：高维稀疏特征是不是可以理解为低维的稠密特征映射到了高维空间。这里联想到了SVM的核技巧，不也是为了将特征由低维空间映射到高维空间中实现特征的线性可分吗？在SVM中已经证实了其有效性。这里面应该存在某种规律，LR在高维空间比低维空间中具有更高的期望实现更好分类效果的。



# 面试题目
1. bagging算法如何增加个体学习器的多样性？
2. boosting和bagging的思想，联系和区别？
3. rf随机性体现？
4. 如何从减小方差和偏差的角度解释Boosting和Bagging的原理?
5. 随机森林如何评估特征重要性?
6. 随机森林中的基分类器，由决策树替换为线性分类器或K-近邻?
7. 最常用的基分类器是决策树的原因？
8. 为什么不稳定的学习器更适合作为基学习器？
9. Bagging方法中能使用线性分类器作为基学习器吗？ Boosting呢？
10. Adaboost的过程？损失函数？
11. 梯度提升算法的自变量是什么？
12. Gradientboosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？
13. GBDT为什么对损失函数求负梯度可以达到拟合残差的作用？
14. 如何控制GBDT过拟合？
15. GBDT的基本原理是什么?
16. 梯度提升和梯度下降的区别和联系是什么？
17. xgb的特点？
18. XGBoost与GBDT的联系和区别有哪些?
19. xgboost怎么给特征评分？
20. Lightgbm的特点？
21. 从机器学习三要素和特征数据角度阐述GBDT与LR的区别？


# 论文
- XGBoost: A Scalable Tree Boosting System， Tianqi Chen

