## 线性模型：
    线性回归，最小二乘，最小角回归LARS，拉索，岭，弹性网
    逐步向前，局部加权，多元自适应回归样条
    保序回归
    非线形回归，广义线性模型（对数线性回归），多项式回归
    huber回归
    非参数回归
    逻辑回归（LR，softmax），最大熵模型MEM(MaximumEntropy Model)
    感知机（Perception）
    支持向量机（SVM，SVR，多类SVM，nu）
    判别分析（LDA，QDA），FLD，菲舍尔判别分析FDA，lmse，SGDClassifier
    非线形判别分析，分段线形判别分析，
    势函数法
    Orthogonal Matching Pursuit model (OMP)
    Passive Aggressive
    RANSAC (RANdom SAmple Consensus) 
    Theil-Sen Estimator
    Adaline 算法


## 线性回归
https://blog.csdn.net/Mr_tyting/article/details/603508151

核心思想
   通过学习获得属性的线性组合来尽可能准确的预测实例输出标记
模型
   属性的线性组合表达
策略
   均方误差损失MSE=方差+偏差的平方
算法
   最小二乘法
   极大似然估计
最小二乘估计
  就是假定误差服从高斯分布，认为样本是独立，使用最大似然估计就能得出的结论
  极大似然估计
	目的：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
	原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。
	极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。
	当XTX为满秩矩阵或正定矩阵时，

梯度下降法
  不是满秩矩阵,则就不存在逆矩阵.或者当特征维度非常大时求逆运算时间开销很大，用梯度下降法对最小二乘法形式的误差损失函数进行优化
  1) 定义一个最大迭代步数,当迭代次数大于等于这个最大迭代步数时,停止迭代
  2) 定义一个最小误差,当你更新得到的w, b代入到均方误差中得出的误差小于定义的最小误差,则停止迭代。
  梯度 = 误差×特征

模型特点应用场景
  缺点
    易欠拟合；
  优点
    实现简单、计算简单、很好解释性

特征多于样本？
	岭回归L2约束
	lasso回归L1约束；
	弹性网回归
	局部加权线性回归；
	逐步向前回归；
    线性模型逼近输出标记的函数；
    多项式回归
    SVR
    RVM
    KNN 回归
    基于决策树
    神经网络

与kNN的关系
	线性模型低方差高偏差，kNN高方差低偏差(kNN改进：核方法用权重；高维空间用更合适的距离度量)
	期望平方损失推出的预测是在一个点处的条件期望，kNN的预测是在一个区域内的期望。
	kNN和最小二乘法都是通过平均来近似条件期望；最小二乘假设决策函数可以用一个全局线性函数近似，而kNN假设用一个局部常数函数来近似；
	kNN在高维中样本稀疏问题；维度灾难，采样密度问题；

偏差和，特征相关性
  线性回归中线性相关的两个特征对线性模型的影响？ill-condition

最小二乘估计和极大似然的关系？
  假定误差服从高斯分布，认为样本是独立，使用最大似然估计就能得出的结论

为什么用梯度下降？
  不是满秩矩阵,则就不存在逆矩阵.或者当特征维度非常大时求逆运算时间开销很大，用梯度下降法对最小二乘法形式的误差损失函数进行优化
 

## 逻辑回归：
https://blog.csdn.net/Mr_tyting/article/details/60573870

核心思想
  找一个单调可微函数，将二分类的真实标记y与回归模型的预测值联系起来。
模型
  属性的线性组合表达预测的对数几率
策略
  对数损失，两种形式
算法
  梯度下降法
  指数族分布：
  	高斯分布属于指数族分布
  	二项分布属于指数族分布
  	泊松分布属于指数族分布
梯度 = 误差×特征

模型特点应用场景
  缺点
   - 易欠拟合；
  优点
   -  容易理解和实现；
   -  计算量小分类快；
   -  存储资源低；
   -  可得到近似概率；
   -  直接对分类可能性建模避免假设分布不准确带来的问题；

损失函数？
  对数似然的相反数加上正则化项，避免过拟合；
	
如何多分类？
  多项Logistic回归，该方法也被称之为softmax函数；在此基础上衍生出来的softmax可以用于多分类
	
最优化算法选择？
  Newton-cg，lbfgs，lib-linear，sag

方差膨胀因子 VIF：Variance inflation factor，一般大于5或10的变量删除
系数的置信度、置信区间、z值、wald卡方、p值，
删除系数为负数的、小的特征，
不平衡数据smote采样
拟合度曲线
最优分箱
特征选择：前向、后向、逐步回归，http://www.cnblogs.com/biyeymyhjob/archive/2012/07/18/2595410.html
损失函数不是平方差损失？在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部 最优值，梯度下降法很可能找不到全局最优值，梯度会消失、平方损失非凸求不到最优解
WOE IV : http://blog.csdn.net/lingan_Hong/article/details/77718123
信用评分卡模型
  http://www.yangqiu.cn/holdyongquan/1353711.html
  http://blog.csdn.net/lll1528238733/article/details/76602006
  http://blog.csdn.net/abc200941410128/article/details/75452277
  http://blog.csdn.net/huobanjishijian/article/details/51611860
  http://blog.csdn.net/Mr_tyting/article/details/75878638
iv woe psi的计算，woe转化的好处（解释性，减少异常点的影响，加快收敛）,每个区间woe和坏账率的关系，概率到分数如何建立映射？

评分卡模型系数有什么要求？显著性？方差膨胀因子 VIF：Variance inflation factor，一般大于5或10的变量删除？ 
损失函数（对数似然求平均的相反数），公式推导出梯度。 
最后的结果梯度 = 误差 * 特征
softmax是如何计算的？ 
当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪些场景，它们之间又有怎样的关系？ 
为什么用sigmoid函数？一是： sigmod 本身的性质。 二是：之所以LR 用sigmod，不是因为LR 选择了 sigmod ，而是用 指数簇分布和 最大熵原理 推导出来的形式，就是这个样子，后来起名叫sigmod
逻辑回归，损失函数，以及如何求参数？LR模型是输出Y=1的对数几率是输入x的线性表示的模型，进而得到输出Y=1的概率是关于输入的线性表示的sigmoid函数；损失函数是对数似然函数的相反数，利用数值优化算法求解最优参数值，最后学得逻辑回归模型为两个概率；推广位多项逻辑回归模型用于多分类。
我知道校正R2或者F值是用来评估线性回归模型的。那用什么来评估逻辑回归模型？　
  - 由于逻辑回归是用来预测概率的，我们可以用AUC-ROC曲线以及混淆矩阵来确定其性能。
  - 此外，在逻辑回归中类似于校正R2的指标是AIC。AIC是对模型系数数量惩罚模型的拟合度量。因此，我们更偏爱有最小AIC的模型。
  - 空偏差指的是只有截距项的模型预测的响应。数值越低，模型越好。残余偏差表示由添加自变量的模型预测的响应。数值越低，模型越好。

极大似然估计的思想？最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

***逻辑回归相比于线性回归，有何异同?
区别：
  逻辑回归处理的是分类问题，线性回归处理的是回归问题；
  逻辑回归因变量取值是一个二元分布，线性回归是正态分布；逻辑回归中的因变量为离散的，而线性回归中的 因变量是连续的，
相同：
  可以认为二者都使用了极大似然估计来对训练样本进行建模；
  二者在求解超参数的过程中，都可以使用梯度下降的方法，这也是 监督学习中一个常见的相似之处。

***当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪 些场景，它们之间又有怎样的关系?
  当存在样本可能属于多个标签的情况时，我们可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第i类标签”与“非第i类标签”两类。通过这样的办法，我们就解决了每个样本可能拥有多个标签的情况。




## 感知机模型
https://blog.csdn.net/yxhlfx/article/details/79093456

核心思想
  通过学习获得属性的线性组合来尽可能准确地根据计算结果的正负预测实例地输出标记。求出将训练数据线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降对损失函数进行极小化。

模型
  属性的线性组合的正负表示分类的正负

策略
  误分类点集的函数距离之和的相反数

算法
 - 随机梯度下降
 - 当样本线性可分时，算法收敛
 - 对偶形式

核技巧

模型特点应用场景
 	场景
 	优点
    可以在线学习
    It does not require a learning rate.
    It is not regularized (penalized).
    It updates its model only on mistakes.

相关模型
  应用到多分类问题中

误分类点到超平面的总距离；为何可以不考虑系数尺度？http://blog.csdn.net/lyg1112/article/details/52572405


## LDA
假设各类样本的协方差矩阵相同，且满秩；
  训练时，寻找一条直线使得同类样本投影点尽可能接近、异类样本点尽可能远；
  预测时，根据新样本投影点的位置确定样本的类别；
  从贝叶斯决策理论角度看，两类数据同先验、满足高斯分布、协方差相等时，LDA可达到最优分类；

属性的线性组合表达投影平面

最大化距离与协方差之比，即类内散度矩阵和类间散度矩阵的广义瑞利熵

拉格朗日乘子法，奇异值分解


## svm
https://blog.csdn.net/Mr_tyting/article/details/60573870
通过学习获得属性的线性组合来尽可能准确的预测实例输出标记

核心思想：SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面（间隔最大是它有别于感知机）

模型：分离平面
策略：正则化的合页损失
算法
  它是定义在特征空间上使间隔最大的线性分类器；
  考虑硬间隔最大化，优化问题为函数间隔不小于1时最大化支持向量到分离平面的几何间隔；
  拉格朗日对偶性，原始问题为极小极大L函数转换为对偶问题极大极小；
  用SMO算法求解该对偶问题；
  根据KKT条件算出原问题的解；
  最后得到决策函数

模型特点
  优点
     可处理线性可分和非线性可分（输入空间映射到希尔伯特空间，定义好核函数就可以，Gram矩阵半正定，）；
     泛化误差小；
     内存效率高；
     结果易解释；
  缺点
     对参数和核函数选择敏感；
     原始SVM擅长处理二分类问题；
     由很少的重要的训练样本决定；

场景
   拿到数据就可以尝试一下；
   新闻分类、手写识别

线性不可分怎么办？
  利用核技巧转化为在高维空间中线性可分；
 
常见核函数：
  多项式；高斯亦称为rbf核；字符串；线性；拉普拉斯；sigmoid核函数

一些样本出错怎么办？
  加入损失函数（0-1损失，hinge损失，指数损失，对率损失），使软间隔最小；

为什么要间隔最大化？
  误分次数不超过(2R/d)^2；

怎么做回归？
  SVR，加入epsilon损失允许有epsilon的残差；

几何间隔?
  实例点到超平面的带符号的距离

硬间隔最大化？
  意味着以充分大的确信度对训练数据进行分类

软间隔最大化？
  加入了松弛变量

SVM核函数理解；为什么KKT条件可以求出参数?
  核函数将输入转换为特征空间中特征向量的内积；
  KKT条件保证转化为对偶问题的求解的合理性

解释对偶的概念?
  一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

SVM与LR的区别？
  SVM分类器只由很少的训练样本确定；

http://blog.csdn.net/szlcw1/article/details/52259668 数据挖掘（机器学习）面试--SVM面试常考问题

SVM、LR、决策树的对比？
  模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;，决策树容易过拟合，需要进行剪枝
  损失函数：SVMhinge loss; LR L2正则化; adaboost 指数损失
  数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
  数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

***在空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在超平面上的投影仍然是线性可分的吗?
  从SVM超平面仅由支持向量决定，直观推导。
  凸优化理论中的超平面分离定理(Separating Hyperplane Theorem，SHT)更加轻巧地解决。该定理描述的是，对于不相交的两 个凸集，存在一个超平面，将两个凸集分离。对于二维的情况，两个凸集间距离 最短两点连线的中垂线就是一个将它们分离的超平面。
  结论是不可分。

***是否存在一组参数使SVM训练误差为0?
  一个使用高斯核训练的SVM中，试证明若给定训练集
中不存在两个点在同一位置，则存在一组参数{α1,...,αm,b}以及参数γ使得该SVM的 训练误差为0。

***训练误差为0的SVM分类器一定存在吗? 存在
***加入松弛变量的SVM的训练误差可以为0吗? 不一定，使用SMO算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由
于我们的优化目标改变了，并不再是使训练误差最小。

## 最大熵模型
在满足约束条件的模型集合中选择熵最大的模型


