# 逻辑回归：

- 方差膨胀因子 VIF：Variance inflation factor，一般大于5或10的变量删除
- 系数的置信度、置信区间、z值、wald卡方、p值，
- 删除系数为负数的、小的特征，
- 不平衡数据smote采样
- 拟合度曲线
- 最优分箱
- 特征选择：前向、后向、逐步回归
http://www.cnblogs.com/biyeymyhjob/archive/2012/07/18/2595410.html
- 在逻辑回归中我们不这么做，因为 当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部 最优值，梯度下降法很可能找不到全局最优值，

WOE IV : http://blog.csdn.net/lingan_Hong/article/details/77718123

信用评分卡模型http://www.yangqiu.cn/holdyongquan/1353711.html
http://blog.csdn.net/lll1528238733/article/details/76602006
http://blog.csdn.net/abc200941410128/article/details/75452277
http://blog.csdn.net/huobanjishijian/article/details/51611860
http://blog.csdn.net/Mr_tyting/article/details/75878638

线性回归的偏差和，特征相关性


52. 线性回归中线性相关的两个特征对线性模型的影响？ill-condition
53. 回归算法：线性回归，广义线性回归，多项式，岭回归，拉索回归，弹性网回归，逐步回归，局部加权线性回归（LWLR），SVR，RVM，KNN 回归，基于决策树，神经网络
54. 线性模型低方差高偏差，kNN高方差低偏差(kNN改进：核方法用权重；高维空间用更合适的距离度量)
55. 期望平方损失推出的预测是在一个点处的条件期望，kNN的预测是在一个区域内的期望。
56. kNN和最小二乘法都是通过平均来近似条件期望；最小二乘假设决策函数可以用一个全局线性函数近似，而kNN假设用一个局部常数函数来近似；
57. kNN在高维中样本稀疏问题；维度灾难，采样密度问题；
58. MSE = 方差+偏差的平方
59. 逻辑回归，损失函数，以及如何求参数？LR模型是输出Y=1的对数几率是输入x的线性表示的模型，进而得到输出Y=1的概率是关于输入的线性表示的sigmoid函数；损失函数是对数似然函数的相反数，利用数值优化算法求解最优参数值，最后学得逻辑回归模型为两个概率；推广位多项逻辑回归模型用于多分类。 
60. 我知道校正R2或者F值是用来评估线性回归模型的。那用什么来评估逻辑回归模型？　1.由于逻辑回归是用来预测概率的，我们可以用AUC-ROC曲线以及混淆矩阵来确定其性能。2.此外，在逻辑回归中类似于校正R2的指标是AIC。AIC是对模型系数数量惩罚模型的拟合度量。因此，我们更偏爱有最小AIC的模型。3.空偏差指的是只有截距项的模型预测的响应。数值越低，模型越好。残余偏差表示由添加自变量的模型预测的响应。数值越低，模型越好。
61. 极大似然估计的思想？最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。
62. 朴素贝叶斯介绍？为什么朴素贝叶斯如此“朴素”？  利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；由训练数据学得联合概率，再得到后验概率；它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
63. LR NB的区别？1.朴素贝叶斯分类器将会比判别模型，譬如逻辑回归收敛得更快，因此你只需要更少的训练数据。2.其主要缺点是它学习不了特征间的交互关系
64. 贝叶斯分类器：依据条件分布将样本分到条件概率最大的类中。
65. 分类器：决策函数形式或条件概率形式
66. 为什么说朴素贝叶斯是高偏差低方差？它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差
67. 解释置信区间？置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%
68. 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？答：我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。
69. KNN三要素？K值选择，距离度量，决策规则
70. kNN时间复杂度？O(n)
71. 优化KNN?使用kd树或者ball tree(这个树不懂),将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可
72. SVM讲解？它是定义在特征空间上使间隔最大的线性分类器；考虑硬间隔最大化，优化问题为函数间隔不小于1时最大化支持向量到分离平面的几何间隔；拉格朗日对偶性，原始问题为极小极大L函数转换为对偶问题极大极小；用SMO算法求解该对偶问题；根据KKT条件算出原问题的解；最后得到决策函数
73. 几何间隔?实例点到超平面的带符号的距离
74. 硬间隔最大化？意味着以充分大的确信度对训练数据进行分类
75. 软间隔最大化？加入了松弛变量
76. SVM核函数理解；常用的核函数；为什么KKT条件可以求出参数?核函数将输入转换为特征空间中特征向量的内积；线性核函数，多项式核函数，高斯核函数，拉普拉斯核函数，Sigmoid核函数;KKT条件保证转化为对偶问题的求解的合理性
77. 解释对偶的概念?一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。
78. SVM与LR的区别？1.SVM分类器只由很少的训练样本确定；
79. http://blog.csdn.net/szlcw1/article/details/52259668数据挖掘（机器学习）面试--SVM面试常考问题
80. SVM、LR、决策树的对比？模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核
81. 决策树的算法包含？特征选择，决策树生成，剪枝（模型选择，避免过拟合，根据最小损失函数原则剪枝）。
82. 决策树属性选择的指标的含义？信息增益（数据集中类与属性A的互信息，表示由于特征A使得对数据集D的分类不确定性减少的程度，偏向取值较多的属性），信息增益率（信息增益与数据关于特征A的熵之比，先选出信息增益高于平均水平的属性），gini指数（表示经过属性A分割后数据集的不确定性或均匀度）
83. 决策树如何回归？最小二乘回归树算法：平方误差最小准则，每个内部节点构建时选择最优切分变量j和切分点S
84. CART剪枝算法用到了后剪枝和验证集（交叉验证）
85. Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？答：最根本的区别是，随机森林算法使用bagging技术做出预测；而GBM是采用boosting技术做预测的。1.在bagging技术中，数据集用随机采样的方法被划分成n个样本。然后，使用单一的学习算法，在所有样本上建模。2.接着利用投票或者求平均来组合所得到的预测。3.bagging是平行进行的，而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在后续一轮中得到校正。这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。4.随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。在另一方面，GBM提高了精度，同时减少了模型的偏差和方差
86. 随机森林如何评估特征重要性（http://charleshm.github.io/2016/03/Random-Forest-Tricks/）?1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。
87. 花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM（Gradient Boosted Models），想着boosting算法会展现“魔力”。不幸的是，没有一个模型比基准模型表现得更好。最后，你决定将这些模型结合到一起。尽管众所周知，结合模型通常精度高，但你就很不幸运。你到底错在哪里？　答：据我们所知，组合的学习模型是基于合并弱的学习模型来创造一个强大的学习模型的想法。但是，只有当各模型之间没有相关性的时候组合起来后才比较强大。由于我们已经试了5个GBM也没有提高精度，表明这些模型是相关的。具有相关性的模型的问题是，所有的模型提供相同的信息。例如：如果模型1把User1122归类为1，模型2和模型3很有可能会做同样的分类，即使它的实际值应该是0，因此，只有弱相关的模型结合起来才会表现更好。
88. 简述Adaboost算法过程？模型为加法模型，策略是指数损失函数（训练误差以指数速率下降），算法为前向分步算法（训练数据初始权重为1/n，训练出一个分类器并得到误差率和该分类器的权重，接着更新训练数据的权值分布或者重采样）由前向分步算法思想，将基本分类器设为决策树（二叉分类树和二叉回归树），回归问题损失函数为平方误差损失，可以转化为拟合残差或损失函数的负梯度在当前模型的值（GDBT），分类问题为指数损失。
89. 讲一讲GBDT，问为什么对损失函数求负梯度可以达到拟合残差的作用？泰勒一阶展开解释。梯度提升法。
90. 如何控制GBDT过拟合？每棵树叶子节点个数在4—8之间；通过验证集来确定树的棵数M的大小（正则化）；通过缩减来正则化（加入学习率v）；随机梯度提升（每个基学习器的采样数为0.5），减少了方差（过拟合）；叶子节点中包含的样本最少个数；加入决策树的惩罚项用于后剪枝（结构风险）（叶子节点个数或者关于叶子节点数的L2惩罚项）
91. 简述Bagging算法过程？随机森林：属性选择扰动；样本扰动（自助采样，有放回），提高基学习器之间的差异性进而提高泛化性能。降低方差，对神经网络、决策树这样的易受样本扰动的学习器上效用更为明显
92. 如何增加基学习器的多样性？数据样本扰动，输入属性扰动，输出表示扰动，算法参数扰动
93. 简述结合策略stacking：初级学习器的输出作为次级学习器的输入
94. xgboost怎么给特征评分？在训练的过程中，通过结构分数的增益情况选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。
95. 聚类算法有哪些；
96. Gaussian clusters?
97. K-means时间复杂度为O(knt) 
98. 选择批次距离尽可能远的K个点；首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个
99. KMeans初始类簇中心点的选取?k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大4. 重复2和3直到k个聚类中心被选出来5. 利用这k个初始的聚类中心来运行标准的k-means算法
100.  采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦
101.  SVD和SVD++:降维，聚类隐形语义索引、信息检索； 图像压缩，推荐系统http://blog.csdn.net/u011412768/article/details/52972081
102.  讲一讲DNN
103.  CNN
104.  LSTM
105.  讲一讲sigmoid和reLU的区别，以及各自应该用在什么情况下
106.  讲一讲sigmoid和softmax的区别
107.  讲高维运动数据处理为什么要用谱聚类，不用别的聚类方法
108.  讲一讲LDA，模拟过程，以及训练和推断过程
109.  情景题
a 现在有一堆车牌信息（例如 京A 12345），然后是其中有百分之二十的车牌有登记时间，现在要把剩下百分之八十的车牌登记上时间，问怎么做
b 现在有人工标记的一些图片，分为五个等级（非常好，好，中，差，非常差），现在需要来预测新的图片（softmax），可是发现预测出的图片有些问题即当非常好的概率为最大时，有时候差的概率是次大的，这是不符合预期的，问应该怎么做 

1.  连续监督学习有什么不同方法？滑动窗口方法2. 复发性推拉窗3. 隐藏马尔科夫模型4. 最大熵马尔科夫模型5. 条件随机域6. 图变换网络。
2.  什么是PAC学习？可能近似正确模型 (PAC) 学习是一个已经被引入到分析学习算法和统计效率的学习框架
3.  有哪些不同的类别可以分为序列学习过程？序列预测2. 序列生成3. 序列识别4. 顺序决定.