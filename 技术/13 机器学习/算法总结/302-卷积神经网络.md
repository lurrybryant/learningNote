## 卷积神经网络CNN：
LeNet-5 架构（1998 年），
ILSVRC 挑战赛的三名获胜者 AlexNet（2012），ZFNet(2013)，
Overfeat
NiN，
GoogLeNet（2014）
ResNet（2015），
VGGnet，
Inception-v4，Xception，Bottleneck layer
Inception V2
Inception V3
Inception-Resnet-V3， 
WRN （Wide Residual Network）2016
SqueezeNet
preResNet，ResNeXt，
DenseNet， 
MobileNet， 
ShuffleNet-V2，
ENet，
FractalNet，
SENet，

至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络 
动机
    稀疏交互(sparse interactions)、 
    参数共享(parameter sharing)、 存储效率和计算效率高。稀疏连接和参数共享是显著 提高线性函数在一张图像上进行边缘检测的效率。使得卷积层具有平移等变性。
    等变表示(equivariant representations)，平移等变：f(g(x)) = g(f(x))  ，
     

池化
    池化函数：使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出 
    最大池化（能够抑制网络参数误差造 成估计均值偏移的现象，特点是更好地提取纹理信息。 ）、相邻矩形区域内的平均值、L2 范数以及基于据中心像素距离的加权平均函数（能 够抑制由于邻域大小受限造成估计值方差增大的现象，特点是对背景的保留效果 更好），对相邻重叠区域的池化以及空间金字塔池化。 相邻重叠区域的池化，顾名思义，是采用比窗口宽度更小的步长，使得窗口在每 次滑动时存在重叠的区域。空间金字塔池化主要考虑了多尺度信息的描述，例如 同时计算1×1、2×2、4×4的矩阵的池化并将结果拼接在一起作为下一网络层的输 入。
    能显著降低参数量外，还能够保持对平移、伸缩、旋转操作的 不变性
    平移不变：局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现 而不关心它出现的具体位置时 
    使用池化可以看作是增加了一个无限强的先验:这一层学得的函数必须具有对 少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。 
    池化对于处理不同大小的输入具有重要作用 
    池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，例如玻尔兹 曼机和自编码器。 

输入图像也由多个子图层组成：每个颜色通道一个。 通常有三种：红色，绿色和蓝色（RGB）。 灰度图像只有一个通道，但是一些图像可能更多 - 例如捕捉额外光频（如红外线）的卫星图像


卷积神经网络如何用于文本分类任务?能够自动地对N- gram特征进行组合和筛选，获得不同抽象层次的语义信息。由于在每次卷积中采 用了共享权重的机制，因此它的训练速度相对较快，在实际的文本分类任务中取 得了非常不错的效果。
输入，卷积，池化，输出

深度残差网络
随着神经网络层数的加深，优化函数越来越陷入局部最优解。同 时，随着网络层数的增加，梯度消失的问题更加严重，这是因为梯度在反向传播 时会逐渐衰减。特别是利用Sigmoid激活函数时，使得远离输出层(即接近输入 层)的网络层不能够得到有效的学习，影响了模型泛化的效果
解决或缓解深层的神经网络训练中的梯度消失问题
涉及非常多的参数和导数的连乘，这时误差很容易产生消失或者膨胀，影 响对该层参数的正确学习。因此深度神经网络的拟合和泛化能力较差，有时甚至 不如浅层的神经网络模型精度更高。ResNet通过调整网络结构来解决上述问题。


架构
    卷积、探测、池化
    典型的 CNN 体系结构有一些卷积层（每一个通常跟着一个 ReLU 层），然后是一个池化层，然后是另外几个卷积层（+ ReLU），然后是另一个池化层，等等。随着网络的进展，图像变得越来越小，但是由于卷积层的缘故，图像通常也会越来越深（即更多的特征映射）。 在堆栈的顶部，添加由几个全连接层（+ ReLU）组成的常规前馈神经网络，并且最终层输出预测（例如，输出估计类别概率的 softmax 层）
    一个常见的错误是使用太大的卷积核。 通常可以通过将两个3×3内核堆叠在一起来获得与9×9内核相同的效果，计算量更少。

发展历史：
LeNet-5 架构
作者： Yann LeCun，1998
手写数字识别（MNIST）
贡献
卷积+池化(下采样)+非线性激活的组合是CNN的
       典型特征
使用卷积提取空间特征
使用映射到空间均值下采样（subsample）
双曲线（tanh）或S型（sigmoid）形式的非线性
多层神经网络（MLP）作为最后的分类器
层与层之间的稀疏连接矩阵避免大的计算成本
从1998年到2010年，神经网络处于孵化阶段
大多数人没有意识到他们不断增强的力量，与此同时其他研究者则进展缓慢。
由于手机相机以及便宜的数字相机的出现，越来越多的数据可被利用。
计算能力也在成长，CPU变得更快，GPU变成了多种用途的计算工具。
数据和计算能力使得神经网络能够完成的任务越来越有趣，之后一切变得清晰起来。
Dan Ciresan Net
2010年，Dan Claudiu Ciresan 和Jurgen Schmidhuber 发布了最早的GPU神经网络的一个实现。这个实现是在一块NVIDIA GTX280图形处理器上运行9层的神经网络，包括前向与反向传播。


AlexNet
作者： Alex Krizhevsky（因此而得名），ImageNet ILSVRC 挑战赛，2012.
贡献
使用修正的非线性单元（ReLU）
数据扩增
在训练的时候使用Dropout技术有选择的忽视单个神经元，从而避免过拟合
覆盖进行最大池化，避免平均池化的平均化效果。
使用GPU NVIDIA GTX580减少训练时间


ZFNet 2013
提出通过反卷积方法进行卷积网络可视化的方法，可以了解到图像“学”到了什么。以此改进，在AlexNet基础上取得了更好的成果。


Overfeat
2013年12月， 纽约大学的Yann LeCun实验室提出了AlexNet的衍生—Overfeat
贡献
学习边界框（learning bounding box） 


VGGNet
作者： Visual Geometry Group， 2014 年 ILSVRC 挑战赛的亚军
VGG-16，VGG-19
贡献
CNN 越来越深入。 他们也越来越轻量，需要越来越少的参数。主要结论就是深度的增加有益于精度的提升
是第一个在各个卷积层使用更小的3*3过滤器（filter），并把他们组合成为一个卷积序列进行处理的网络。
与AlexNet相比：
相同点
整体结构分五层；
除softmax层外，最后几层为全连接层；
五层之间通过max pooling连接。
不同点
使用3×3的小卷积核代替7×7大卷积核，网络构建的比较深；
由于LRN太耗费计算资源，性价比不高，所以被去掉；
采用了更多的feature map，能够提取更多的特征，从而能够做更多特征的组合。
1.LRN层太耗费计算资源，作用不大，可以舍去。　  
2.大卷积核可以学习更大的空间特征，但是需要的参数空间也更多，小卷积核虽然学习的空间特征有限，但所需参数空间更小，多层叠加训练可能效果更好。
3.越深的网络效果越好，但是要避免梯度消失的问题，选取relu的激活函数、batch_normalization等都可以从一定程度上避免。
4.小卷积核+深层网络的效果，在迭代相同次数时，比大卷积核+浅层网络效果更好，对于我们自己设计网络时可以有借鉴作用。但是前者的训练时间可能更长，不过可能比后者收敛速度更快，精确度更好。
但是，VGG简单的堆叠卷积层，而且卷积核太深(最多达512)，特征太多，导致其参数猛增，搜索空间太大，正则化困难，因而其精度也并不是最高的，在推理时也相当耗时，和GoogLeNet相比性价比十分之低。



Network-in-network
2013年底
使用1*1卷积为卷积层的特征提供更组合型的能力。
NiN架构在各个卷积之后使用空间MLP层，以便更好地在其它层之前组合特征。同样，你可以认为11卷积与LeNet最初的原理相悖，但是事实上他们可以以一种更好的方式组合卷积特征，而这时不可能通过简单的堆叠更多的卷积特征做到的。这和使用原始像素作为下一层输入是有区别的。其中11卷积常常被用于在卷积之后的特征映射上对特征进行空间组合，所以它们实际上可以使用非常少的参数，并在这些特征上的所有像素上共享。
MLP的能力是通过将卷积特征组合到更复杂的组（group）来极大地增强单个卷积特征的有效性。这个想法之后被用到一些最近的框架上，例如ResNet，Inception及其衍生技术。
NiN也使用了平均池化层作为最后分类器的一部分，这是另一种将会变得常见的实践。



GoogLeNet
2014年秋，作者： Christian Szegedy 等人，ImageNet ILSVRC 挑战赛
GoogLeNet 的参数比 AlexNet 少了 10 倍（约 600 万而不是 6000 万）
初始模块可以视为类固醇卷积层，能够输出捕捉各种尺度复杂模式的特征映射
Inception-v1，Inception的伟大思路在于1*1的卷积块（NiN）在昂贵的并行模块之前减少特征的数量。这一般被称之为瓶颈（bottleneck）
Bottleneck layer
Inception V2
2015年2月，Batch-normalized Inception
Inception V3
2015年12月，
Inception V4，2016
Xception



ResNet
作者： Kaiming He 等人 开发
 ILSVRC 挑战赛，2015.12
由 152 层组成。 能够训练如此深的网络的关键是使用跳过连接（skip connection，也称为快捷连接）：一个层的输入信号也被添加到位于下一层的输出。
VGGNet和 Inception-v4（将 GoogLeNet 和 ResNet 的思想融合在一起，实现了接近 3% 的 top-5 误差 ImageNet 分类率）



Inception - Resnet V1/V2 2016
基于Inception-V3 和 Inception-V4及残差网络的思想的融合
NASNet 
通过强化学习自动产生结构
WRN （Wide Residual Network）2016
ResNext 2016
提出原因：对于网络的改进，传统的方法主要增加深度或者增加宽度，但都会大大增加设计难度和复杂度方法：同时基于思想，以及Inception的split-transform-merge思想，提出了该扩展性较强的网络。提到了一个名词cardinality（基数），个人理解即一个模块中同时并列多个子网络。结论：Cardinality越大越好，且在同样的复杂度下，比增加深度和宽度效果要好
DenseNet
对小数据结果较好，特征图会传到网络后面的所有层
MobileNet
用于嵌入式设备，牺牲较少的精度换取减少大量的计算。总结：对卷积神经网络结构的改进主要在卷积层，而改进方法主要有有以下几种：卷积核小型化（参数减少），1x1卷积（升降维），Network In Network，Inception机制，卷积分解（Factorization），反卷积运算（实验可用以调参）。此外，对网络结构其他部分常用的方法要理解透彻，例如优化器的各种优化方法，激活函数损失函数的选择等，读者可以自己详读开篇的文章。



SqueezeNet
该架构对ResNet与Inception里面的概念进行了重新的处理。一个更好的架构设计网络型号要小，而且参数还不需要复杂的压缩算法。
ENet
Adam Paszke，ENet 是一个编码加解码的网络，将分类反向传播给原始图像进行分割。这只使用了神经网络，没有其他算法进行图像分割。
ENet被设计为在开始时尽可能使用最小数量的资源。正是因为它有着如此小的脚本，编码器和解码器网络共占有0.7MB 16fp的精度。即使这么小的型号，ENet在分割准确度上也类似于或高于其他神经网络的解决方案。



模块分析1）使用没有batch norm的ELU非线性或者有batch norm的ReLU。2）使用一个学习到的RGB的彩色空间转换。3）使用线性学习率衰退策略4）使用平均和最大池化层的和5）使用大约128到256的mini-batch大小。如果这对你的GPU太大，将学习率按比例降到这个大小就行。6）使用完全连接层做为卷积，并为最后的预测平均左右预测。7）当研究增加训练集大小的时候，检测有一个plateau是否没有达到8）数据的整洁要比数据大小重要9）如果你不能增加输入图像的代销，在随后的层上减少步幅（stride），这样做同样的效果。10）如果你的网络有复杂和高度优化的架构，像是GoogLeNet，那修改一定要谨慎。
其他值得关注的架构FractalNet：ResNet的衍生或者更通用的ResNet。
SENet：2017年ImageNet夺冠架构


先验
先验被认为是强或者弱取决于先验中概率密度的集中程度 
卷积：这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求除了那些处在隐藏单元的小的空间连续 的接受域内的权重以外，其余的权重都为零。 
池化：每一个单元都具有对少量平移的不变性 
卷积和池化可能导致欠拟合 ：当一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所利 用的先验可能就不正确了 
当我们比较卷积模型的统计学习表现时，只能以基准中的其 他卷积模型作为比较的对象。
结构化输出
对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使用相邻 像素之间的交互来修正该原始猜测。 
数据类型
可以处理具有可变的空间尺度的输入 
使用卷积处理可变尺寸的输入，仅对输入是因为包含对同种事物的不同 量的观察 (时间上不同长度的记录，空间上不同宽度的观察等) 而导致的尺寸变化这 种情况才有意义 



高效的卷积算法
选择适当的卷积算法来加速卷积 
设计更快的执行卷积或近似卷积，而不损害模型准确性的方法，是一个活跃的 研究领域。甚至仅提高前向传播效率的技术也是有用的，因为在商业环境中，通常部署网络比训练网络还要耗资源。 
随机或无监督特征



历史和神经科学基础
卷积网络是第一批能使用反向传播有效训练的的深度网络之一
基本卷积函数的变体




