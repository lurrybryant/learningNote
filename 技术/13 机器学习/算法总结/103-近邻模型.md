- 近邻模型：
k近邻（kNN）
基于案例推理

1	核心思想
基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个邻居的信息进行预测。
2	模型
三要素：K值选择，距离度量，决策规则
本质：根据三要素将特征空间划分成子空间
错误率不超过最优贝叶斯错误率的2倍
3	策略
4	算法
  kd树实现将复杂度降为O(logN)；
  噪声和非相关性特征向量的存在会使K近邻算法的准确性减小
5	模型特点应用场景
手写数字识别；卫星图片；推荐算法；
优点：
  可用于非线性分类，边界不规范分类；
  训练复杂度为O(n)；
  对数据没有假设；
  对离群点不明显；
缺点：
  计算量大；
  样本不平衡时；
  在取离散值时效果不好；
  需要大量样本，内存大；
  判别模型方差高偏差低；
6	相关模型

7	补充
如何选择k和距离度量？  
交叉验证；马氏距离和欧氏距离；Lp距离，闵式距离
k值反映了近似误差和估计误差的权衡；

线性模型低方差高偏差，kNN高方差低偏差(kNN改进：核方法用权重；高维空间用更合适的距离度量)
kNN在高维中样本稀疏问题；维度灾难，采样密度问题；

在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离，为什么不用曼哈顿距离？答：我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。

kNN时间复杂度？O(n)

优化KNN?使用kd树或者ball tree(这个树不懂),将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做一次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可


# 面试题目
<!-- kNN -->
1. kd树的原理？
2. kNN三要素？
3. 曼哈顿距离和欧氏距离？



