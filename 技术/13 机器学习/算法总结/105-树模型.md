## 决策树
- 树模型：
ID3，C4.5，CART，
Decision Stump，
SLIQ，
Chi-squared Automatic Interaction Detection(CHAID)，
C5.0，

1	核心思想
基于树结构进行决策，每个叶节点对应一个决策结果，每个内部结点对应一个属性测试；采用启发式方法选择当前最优特征；
2	模型
树结构模型
3	策略
正则化的极大似然估计函数
4	算法
特征选择，决策树生成，剪枝（预剪枝训练开销小，带来欠拟合的风险；后剪枝欠拟合风险小，训练开销大）
5	模型特点应用场景
5.1	缺点
  易过拟合；
5.2	优点
  能处理不相关特征；
  能处理有缺失属性的样本；
  可以分类和回归；
  模型可读性强，解释性强；
  分类速度快；
5.3	场景
能处理数值型和标称型
6	相关模型
ID3：信息增益（类与特征的互信息），直到gain很小或者没有特征选择为止；
缺点：
a)	离散值；
b)	无缺失值；
c)	易过拟合；
d)	偏向取值较多的特征；

C4.5：信息增益率，先从候选属性中找出高于平均水平信息增益的属性。
优点：
  对连续值和缺失值处理；
  避免ID3出现过拟合的特性，提升决策树的泛化能力。
缺点：
  偏向取值较少的特征；

Cart：Gini指数
  二叉树，验证集剪枝，后剪枝
a)	回归用平方误差最小化准则，选择最优切分变量j，和切分点s，最后将输入空间划分为M个区域；
b)	分类用基尼系数最小化准则，选择使划分后系数最小的属性，直到小于epsilon为止；

- ID3, C4.5, CART(分类和回归的区别)的原理和优缺点？
 1. 特征类型：ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量；
 2. 缺失值：ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；
 3. 分支个数和特征重复：ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，因此最后会形成一颗二叉树，且每个特征可以被重复使用；
 4. 泛化：ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比。
 5. 回归：CART不仅可以用于分类，也可以应用于回归任务(回归树使用最小平方误差准则)。

7	补充
（1）	决策树的算法包含？
特征选择，决策树生成，剪枝（模型选择，避免过拟合，根据最小损失函数原则剪枝）。
（2）	过拟合怎么办？
加入表示树结构复杂度的量：深度或叶子节点包含的样例数；
损失函数： https://www.zhihu.com/question/34075616/answer/94227516
（4）	决策树属性选择的指标的含义？
信息增益（数据集中类与属性A的互信息，表示由于特征A使得对数据集D的分类不确定性减少的程度，偏向取值较多的属性），
信息增益率（信息增益与数据关于特征A的熵之比，先选出信息增益高于平均水平的属性），
gini指数（表示经过属性A分割后数据集的不确定性或均匀度）
（5）	决策树如何回归？
最小二乘回归树算法：平方误差最小准则，每个内部节点构建时选择最优切分变量j和切分点S
CART剪枝算法用到了后剪枝和验证集（交叉验证）
剪枝方法（预剪枝、后剪枝）代价复杂度剪枝
- 斜决策树（多变量决策树）
- 预剪枝，预剪枝存在一定局限性，有欠拟合的风险
(1)当树到达一定深度的时候，停止树的生长。
(2)当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
(3)计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。
- 后剪枝也可以通过在测试集上的准确率 进行判断，如果剪枝过后准确率有所提升，则进行剪枝。相比于预剪枝，后剪枝 方法通常可以得到泛化能力更强的决策树，但时间开销会更大。
错误率降低剪枝(Reduced Error Pruning，REP)、悲观剪枝(Pessimistic Error Pruning，PEP)、代价复杂度剪枝(Cost Complexity Pruning，CCP)、最小误差剪枝(Minimum Error Pruning，MEP)、CVP(Critical Value Pruning)、OPP(Optimal Pruning)等方法，
属性选择可重复？https://www.zhihu.com/question/56159025
https://blog.csdn.net/Mr_tyting/article/details/60573870

# 面试题目
<!-- 决策树 -->
1. 决策树如何回归？
2. 属性选择是否可重复？
3. 3种算法的区别？
4. 决策树怎么避免过拟合？
5. 剪枝的过程？

