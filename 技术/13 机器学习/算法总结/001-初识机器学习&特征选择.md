## 发展概况
数据
  收集、存储，传输，处理、分析、预测
  大数据时代1.0：数据的积累和呈现，数据即财富
  大数据时代2.0：机器学习用历史数据预测未来，给数据赋予价值

人工智能
  50s-70s初：推理期（逻辑理论）
  70s中期：知识期（专家系统，知识工程）
  80s：符号学习（符号主义，包含决策树（信息论）和基于规则的学习（逻辑论））
  90s中期：统计机器学习（支持向量机SVM及核技巧，未来：符号智能和统计智能相结合）
  21世纪初：深度学习
    需要很强的硬件支持，理论创新不明显，应用范围有限
    50s中后期就有神经网络连接主义
    黑箱，参数过多，计算复杂度大，数据量大
    语音，文本，视频，图像

学习的能力是智能的本质


## 机器学习
阿瑟·萨缪尔（Arthur Samuel）
  跳棋程序，用到增强学习技术
  人工智能领域最能体现智能的一个分支

定义
  学习：一个系统能够通过执行某个过程改进它的性能（Herbert A.Simon）
  机器学习：机器学习研究如何通过计算利用数据（经验E）产生模型（model），使系统在任务T上获得性能P的提高。机器学习是研究关于“学习算法”的学问
  三要点：Experience, Performance, Task
  统计学习：计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析，基本假设前提是，同类数据具有一定的统计规律性（特征与标记具有联合概率分布），且满足独立同分布条件
  机器学习与数据挖掘的区别？
    机器学习是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。
    数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的模式。在这个过程中应用了机器学习算法，另外还用到数据库，数据仓库，统计学，信息检索。

分类
  是否在人类监督下进行训练
    监督学习（分类，回归，标注、排序、Seq2Seq）
    无监督学习（聚类，降维，关联规则、自编码、生成模型、推荐）
    半监督学习
    增强学习
    主动学习
  是否可以动态渐进学习（在线学习 vs 批量学习）
  是否只是通过简单地比较新的数据点和已知的数据点，或者在训练数据中进行模式识别，以建立一个预测模型（基于实例学习 vs 基于模型学习）
  概率模型和非概率模型，线性模型和非线性模型，参数化模型与非参数化模型
  技巧：贝叶斯学习和核方法

思想
  拟合能力  泛化能力
  归纳偏好：机器学习在学习过程中对某种类型假设（假设空间）的偏好（尽可能特殊或者尽可能光滑）
  奥卡姆剃须刀：若多个假设和观察一致，则选择最简单的那个
  算法的归纳偏好和问题本身匹配程度决定了算法能否取得好的性能
  没有免费的午餐定理：所有学习算法的期望性能相同，脱离具体问题空谈“哪个学习算法更好”没有意义。

监督学习
  基本假设：(X, y)具有联合概率分布
  一个监督学习方法包含三要素：
  模型：模型的假设空间，概率模型（条件概率分布）和非概率模型（决策函数），
    假设空间：假设要学习的模型属于某个函数集合
  策略：模型选择的准则
  	损失函数：度量一次预测的好坏；
  	期望风险：度量平均意义下预测的好坏；
  	经验风险最小化：大数定律意义下估计期望风险；但是样本容量小时，会产生过拟合；例如MLE
  	结构风险最小化：加上表示模型复杂度的正则化项（惩罚项）；防止过拟合；例如MAP https://zhuanlan.zhihu.com/p/32480810
  	常用损失函数：1-1；平方；绝对；对数损失（对数似然损失，交叉熵损失）；指数损失；合页损失；感知机损失；
  	http://blog.csdn.net/heyongluoyao8/article/details/52462400
  算法：模型学习的算法，解析解或数值计算方法最优化
    常用数值优化方法？
  	mini-batch （部分增量更新）
  	full-batch（全增量更新）
  	随机梯度
  	牛顿法，
  	拟牛顿法，http://blog.csdn.net/u012328159/article/details/51613262

三空间
  输入空间（样本空间）
  特征空间
  输出空间（标记空间）

三大正则
  L1
    能求得稀疏解，稀疏的好处是特征选择、可解释性强；
  L0
    非零参数个数；
  L2
    可以看成是权值的高斯先验；
    让求解变得稳定；
    数值上更容易求解；
    控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。
减小参数空间；参数空间越小，复杂度越低。 
系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。

挑战
  模型=数据+算法
  数据更重要还是算法更重要？ It depends
  数据
    训练数据量不足
    没有代表性的训练数据(样本偏差受到采样方法和样本容量的影响)
    低质量数据（错误、异常值和噪声）
    不相关的特征（特征选择，特征提取，收集新特征）
    过拟合训练数据
    欠拟合训练数据
  算法


## 建模流程
抽象成数学问题
  X: features（哪些能用，哪些不能用？）
  y: label（如何定义？）
  分类 or 预测

获取训练数据
  设计特征
  带有表现（y）的数据
  数据决定上限，算法逼近上限
  数据要有代表性，与场景同分步、充分充足，否则必然会过拟合。训练集合（模型训练）验证集合（模型选择，用于调整参数，选择特征和做出其他算法相关决定的数据集）测试集合（模型评估）
  对于分类问题，数据偏斜不能过于严重（unbalanced），不同类别的数据数量不要有数个数量级的差距。
  数据不平衡问题：
    阈值移动，
    上采样(smote，有放回)，
    下采样（EasyEnsemble），
    代价敏感学习（修改权重）
  对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法，或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

探索数据
  可视化数据，发现规律

数据预处理
  缺失值？
    缺失值较多，直接将该特征舍弃掉，否则可能会带入较大的noise，对结果造成不良影响。
    缺省值适中，单独当做一类；
    缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理，比如，用全局常量填充；使用与给定元组同一类的所有样本的属性均值或中位数填充；
    使用最可能的值填充；用随机森林等算法预测填充；在保证原有数据样本分布不变情况下进行随机填充；拉格朗日插值法
  元组重复检测？
  数据冲突检测与处理？
  数据光滑技术？ 分箱离散化，回归，离群点分析，聚集

特征变换
  使用sklearn做单机特征工程：http://m.blog.csdn.net/MrLevo520/article/details/78085650
  归一化：http://blog.csdn.net/xbmatrix/article/details/56695825
  数据规约？
    聚集删除冗余特征；
    聚类；
    维归约（数据压缩小波变换 属性子集选择 属性构造 PCA数值归约）；
    参数（回归 对数线性模型）;
    非参数（直方图 距离 抽样 数据立方体聚集）
  压缩数据：分有损和无损压缩；
  规范化；
  离散化（按频次切分分箱bin，单位向量化；onehot编码或叫哑编码；映射到高维空间，降低过拟合）；
  由标称数据产生概念分层；
  数据光滑；
  聚集；
  组合特征；
  属性构造（基于多项式，基于指数，基于对数）；

特征选择
  what?
    选出相关特征，刨除无关特征和冗余特征。
    特征选择做得好，能使非常简单的算法也能得出良好、稳定的结果。
  why?
    去除冗余特征，减少特征数量、降维，减轻维数灾难的问题，减轻学习的负担，使模型泛化能力更强，减少过拟合;
    去掉无关特征，减少不相关特征引入对模型造成干扰，降低学习的难度；
    增强对特征的理解
  how?
    筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。
    过滤（对发散性（方差选择）和相关性评分（皮尔逊相关系数和距离相关系数，卡方检验，互信息，最大信息系数，iv值，条件熵，后验概率，逻辑回归权重等），Relief和Relief-F）
    包裹（根据预测效果评分，递归消除特征，生成与搜索-评价（信息熵、AIC、），拉斯维加斯包裹式选择LVW）
    嵌入（基于惩罚项，基于树模型（前向子集搜索、信息熵评价））
    基于模型（模型得分和重要性，随机森林、XGBoost等）
    稳定性选择（基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）

特征提取
  PCA是为了让映射后的样本具有最大的发散性（无监督）；
  LDA是为了让映射后的样本有最好的分类

训练模型与调优
  选择学习模型：训练时间、预测时间、内存消耗、理解和调参；
     线性分类器速度快、编程方便，但是可能拟合效果不会很好；
     非线性分类器编程复杂，但是特征比数据量还大时效果拟合能力强；
     模型融合；
  选择学习策略：选择合适的正则化；正则化系数c；损失函数，给不同权值；
  实现求解最优模型的算法：最优化算法选择；收敛的阈值或迭代次数；

模型诊断
  如何确定模型调优的方向与思路呢？
  欠拟合：
    增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
    增加模型复杂度，尝试非线性模型，比如核SVM 等模型;
    增加训练轮数
  过拟合：
    增加数据量，正确抽样，减少噪声干扰（对于按区间离散化的特征，增大划分的区间），数据扩增（假设便是，训练数据与将来的数据是独立同分布的，扩增方法：从数据源头采集更多数据；复制原有数据并加上随机噪声；重采样；根据当前数据集估计数据分布参数，使用该分布产生更多数据等）；
    对样本进行降维；
    降低模型复杂度，参数过多，模型复杂度比真模型高，选择简单的模型；
    控制训练轮数，比如最优化求解时，收敛之前停止迭代；决策树剪枝；神经网络在模型对训练数据集迭代收敛之前停止迭代来防止过拟合；集成学习计算验证集的精确度，不再提高就停止训练）；
    集成学习；
  误差分析：
    通过观察误差样本，全面分析误差产生的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题。诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试，进而达到最优状态。

模型评估
  回归：
    均方误差；
  分类：
    错误率和精度；和为1；
    查准率和查全率、F1
    混淆矩阵，PR曲线，平衡点，
    roc：受试者工作特征曲线；
    ks：
    auc：Roc曲线下的面积，面积大的性能好；真实为正例中预测为正例的比例，真实为反例中预测为正例的比例  http://www.jianshu.com/p/6ffa3df3ec86
    当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。
    代价敏感错误率和代价曲线：各个错误代价不一样；
  评估方法
    泛化误差角度考虑如何选择模型
      交叉验证：简单交叉；S折交叉验证；留一法交叉验证；
      自助采样：
    调参和最终模型：
    偏差和方差：期望泛化误差=偏差+方差+噪声；
      数据集小：选高偏差低方差的简单分类器，生成模型；
      数据集大：选低偏差高方差的复杂分类器，判别模型；
    泛化性能是由学习算法的能力、数据的充分性、学习任务的本身难度决定；

模型融合
  一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
  工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

上线部署
  模型在线上运行的效果直接决定模型的成败。 
  准确程度、误差等情况
  运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

监控观察
  编写监控代码，以固定间隔检测系统的实时表现，当发生下降时触发报警
  模型会随着数据的演化而性能下降，除非模型用新数据定期训练
  你还要评估系统输入数据的质量。有时因为低质量的信号（比如失灵的传感器发送随机值，或另一个团队的输出停滞），系统的表现会逐渐变差，但可能需要一段时间，系统的表现才能下降到一定程度，触发警报。如果监测了系统的输入，你就可能尽量早的发现问题。对于线上学习系统，监测输入数据是非常重要的。
  最后，你可能想定期用新数据训练模型。你应该尽可能自动化这个过程。如果不这么做，非常有可能你需要每隔至少六个月更新模型，系统的表现就会产生严重波动。如果你的系统是一个线上学习系统，你需要定期保存系统状态快照，好能方便地回滚到之前的工作状态。
  分析结果
    特征维度
    订单维度


## 附录
训练集和测试集：
  q: 在测试集上多次测量了推广误差率，调整了模型和超参数，以使模型最适合这个集合。这意味着模型对新数据的性能不会高。
  a: 这个问题通常的解决方案是，再保留一个集合，称作验证集合。用测试集和多个超参数训练多个模型，选择在验证集上有最佳性能的模型和超参数。当你对模型满意时，用测试集再做最后一次测试，以得到推广误差率的预估.
  best_method: 为了避免“浪费”过多训练数据在验证集上，通常的办法是使用交叉验证：训练集分成互补的子集，每个模型用不同的子集训练，再用剩下的子集验证。一旦确定模型类型和超参数，最终的模型使用这些超参数和全部的训练集进行训练，用测试集得到推广误差率。
  在机器学习中，我们通常将样本分成训练集，验证集和测试集三部分，数据集规模相对较小，适用传统的划分比例，数据集规模较大的，验证集和测试集要小于数据总 量的20%或10%
  训练集上表现好，但是测试集上表现不好：
    - 你过拟合(Overfit)了开发集；
    - 测试集的数据比开发集数据要更加复杂，你的算法已经达到了预期的效果并且已经无法进一步改善了；
    - 测试集的数据并不比开发集数据复杂，只是因为服从不同分布，所以开发集上良好的性能表现并不能泛化到训练集中。在这种情况下，你在开发集上所做的努力就全部白费了。
    - 当然，构建能够在一种分布中表现良好而且能泛化到其他分布的学习算法是很重要的研究方向。但是，如果你的目标是构建出能在特定的机器学习应用中取得进展的话，我建议你尝试选择服从同一分布的开发集和测试集，这会使您的团队更有效率。


偏差和方差：
  一般来说，最优误差也被称为贝叶斯误差，偏差or方差？
    - 方差高：采用更多数据，正则化（本质是权重接近于0，使得网络变得简单；参数小导致激活函数近似线性）
    - 偏差高：加深网络结构，增加迭代次数，
  Batch Norm：使神经网络对超参数的选择更加稳定，在激活之前使用
  测试集上的评估结果假设检验模型检验：二项检验、t检验
  理解偏差和方差的两个关键数据是训练集误差(Train set error)和验证集误差
  什么时候改变？
    如果你当前的指标和当前用来评估的数据和你真正关心必须做好的事情关系不大，那就应该更改你的指标或者你的开发测试集，让它们能更够好地反映你的算法需要处理好的数据。
    如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。
  训练误差减小，测试误差先小后大：训练集拟合得很好但验证集上预测效果不好
  基于回归任务的期望泛化误差？
    期望泛化误差=偏差+方差+噪声；偏差描述算法本身的拟合能力，方差刻画数据扰动造成的影响，噪声刻画问题本身的难度。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。
解决bias和Variance问题的方法：交叉验证。
  High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征。
  High Variance解决方案：bagging、简化模型、降维
  你意识到你的模型受到低偏差和高方差问题的困扰。那么，应该使用哪种算法来解决问题呢？答：可以使用bagging算法（如随机森林）。因为，低偏差意味着模型的预测值接近实际值，换句话说，该模型有足够的灵活性，以模仿训练数据的分布。这样貌似很好，但是别忘了，一个灵活的模型没有泛化能力，意味着当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。
  另外，为了应对大方差，我们可以：
    - 使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。
    - 使用可变重要性图表中的前n个特征。可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。
什么是偏差和方差?
  偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出 的平均值和真实模型输出之间的偏差。偏差通常是由于我们对学习算法做了错误 的假设所导致的
  方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出 的方差。方差通常是由于模型的复杂度相对于训练样本数m过高导致的，
概念
  QR-decomposition(QR分解)
  L2.5正则

历史
  SVM将手写数字识别错误率降低到0.8%
  2006年，Geoffrey Hinton等人发表了一篇论文展示了如何训练能够识别具有最新精度（>98%）的手写数字的深度神经网络。他们称这种技术为“Deep Learning”。
  TensorFlow 是使用数据流图进行分布式数值计算的更复杂的库。它通过在潜在的数千个多GPU服务器上分布式计算，可以高效地训练和运行非常大的神经网络。TensorFlow 是被 Google 创造的，支持其大型机器学习应用程序。于 2015年11月开源。
 
数据预处理之校准方法？概率校准 
  普拉特校准，保序回归

标准化与归一化的区别？
  标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。
  归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：

数据预处理之归一化(归一到-1,1区间内)标准化（标准正态分布，Z-score）区间缩放（MinMax到0,1区间内）处理好处？ 
  归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。归一化后加快了梯度下降求最优解的速度。等高线变得显得圆滑，在梯度下降进行求解时能较快的收敛。如果不做归一化，梯度下降过程容易走之字，很难收敛甚至不能收敛
  把有量纲表达式变为无量纲表达式, 有可能提高精度。一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）
  逻辑回归等模型先验假设数据服从正态分布。
  哪些机器学习算法不需要做归一化处理？概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf、adaboost、gbdt、xgboost；svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
特征向量的归一化方法？
  线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)
  对数函数转换，表达式如下：y=log10 (x)
  反余切函数转换，表达式如下：y=arctan(x)*2/PI减去均值，
  乘以方差：y=(x-means)/ variance
  特征工程之对定量特征二值化，确定阈值


常见的生成模型：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等；

## 特征工程
从数据分布的角度来讲，做监督学习的基本假设是什么？三要素是什么？从这三个角度思考在数据输入固定的情况下怎么调优模型？
  - 同类数据具有一定的统计规律性（特征与标记具有联合概率分布），且满足独立同分布条件
  - 模型：模型的假设空间，概率模型（条件概率分布）和非概率模型（决策函数）
     策略：模型选择的准则
     算法：模型学习的算法，解析解或数值计算方法最优化
  - 选择学习模型：
     训练时间；预测时间；内存消耗；理解和调参；
     线性分类器速度快、编程方便，但是可能拟合效果不会很好；
     非线性分类器编程复杂，但是特征比数据量还大时效果拟合能力强；
     模型融合；
  - 选择学习策略：选择合适的正则化；正则化系数c；损失函数，给不同权值；
  - 实现求解最优模型的算法：最优化算法选择；收敛的阈值或迭代次数；

为什么深度学习能在大规模数据下比传统机器学习算法能发挥更有效的作用？
  答：？

特征变换或者说特征衍生有用到哪些方法？
  - 基于函数运算、
  - 组合特征（基于决策树）、
  - 离散化分箱、
  - 类别型变量编码（序号，独热，二进制）、
  - 聚类、PCA降维

为何需要特征选择？如何特征选择？
  - 去冗余、去无关、增强解释性、降低维度减轻学习负担、减少过拟合
  - 理解业务，过滤式、包裹式、嵌入式、基于模型、基于稳定性

特征提取的方法？
  - PCA和LDA降维：PCA是为了让映射后的样本具有最大的发散性（无监督）；LDA是为了让映射后的样本有最好的分类
  - 神经网络

哪些机器学习算法需要做归一化处理？为什么要做归一化？
  - 逻辑回归等模型先验假设数据服从正态分布。
  - 避免受尺度较大的特征影响。
     加快梯度下降求最优解的速度

稀疏型数据更适合什么样的模型？
  - Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。
  - GBDT更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低维稠密特征，进一步引入了语意信息有利于特征的表达。

有哪些文本表示模型？
  词袋模型TF-IDF（单词频率*单词对文档的重要性），n-gram模型是多个单词组合成词组，主题模型，词嵌入模型，Word2Vec（cbow, skip-gram, LDA）
  lda和词嵌入的区别？
  由于Softmax激活函数中 存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了Hierarchical Softmax和Negative Sampling两种改进方法
  首先，LDA是利用文档中单词的共现关 系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的Word2Vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。需要说明的是，上述分析的是LDA与Word2Vec的不同，不应该作为主题模型和词嵌入两类方法的主要差异。
  主题模型通过一定的结构调整可以基于“上下文-单词”矩阵进行主题推理。同样地，词嵌入方法也可以根据“文档-单词”矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量即主题);而词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。


数据维度高时选择什么样的分类器？
  线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分
  下面是吴恩达的见解：
    如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 
    如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+GaussianKernel 
    如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

ill-condition病态问题？
  训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）

L1求解？最小角回归算法：LARS算法

什么是组合特征?如何处理高维组合特征? 将用户和物品分别用k维的低维向量表示

在图像分类任务中，训练数据不足会带来什么问题?如何缓解数据量不足带来的问题?
  数据扩充：
    旋转、平移、缩放、裁剪、填充、左右翻转；
    对图像中的像素添加噪声扰动；
    颜色变换；
    改变图像的亮度、清晰度、对比度、锐度；
    在图像 的特征空间内进行变换，利用一些通用的数据扩充或上采样技术；
    生成模型也可以合成一些新样本；
    借助已有的其他模型或数据来进行迁移学习；


## 模型评估
验证集和测试集的作用是什么？

交叉验证和自助采样？

诊断模型时如何判断过拟合、欠拟合？怎么解决？
 - 数据两个维度、
 - 模型、
 - 训练过程、
 欠拟合：
   增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
   增加模型复杂度，尝试非线性模型，比如核SVM 等模型;
   增加训练轮数
 过拟合：
   增加数据量，正确抽样，减少噪声干扰（对于按区间离散化的特征，增大划分的区间），数据扩增（假设便是，训练数据与将来的数据是独立同分布的，扩增方法：从数据源头采集更多数据；复制原有数据并加上随机噪声；重采样；根据当前数据集估计数据分布参数，使用该分布产生更多数据等）；对样本进行降维；
   降低模型复杂度，参数过多，模型复杂度比真模型高，选择简单的模型；集成学习；
   控制训练轮数，比如最优化求解时，收敛之前停止迭代；决策树剪枝；神经网络在模型对训练数据集迭代收敛之前停止迭代来防止过拟合；集成学习计算验证集的精确度，不再提高就停止训练）；

精确率和召回率、混淆矩阵、roc曲线怎么画的（当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动，大数据下怎么算auc？）、PR曲线是什么
 - 真实为正例中预测为正例的比例，真实为反例中预测为正例的比例
 - mAP：评价精度均值

评价指标的局限性：准确率；召回和精度；平方根误差；

期望泛化误差 = 偏差+方差+噪声？
  泛化性能是由学习算法的能力、数据的充分性、学习任务的本身难度决定；

生成模型和判别模型的特点？
  数据集小：选高偏差低方差的简单分类器，生成模型；
  数据集大：选低偏差高方差的复杂分类器，判别模型；

余弦距离、K-L散度是否是一个严格意义上的距离？
  为什么要用余弦距离？
    当一对文本相似度的长度差距很大、但内容相近时，如果使用词频或词向量作为特征，它们在特征空间中的的欧氏距离通常很大;而如果使用余弦 相似度的话，它们之间的夹角可能很小，因而相似度高。此外，在文本、图像、视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。
    欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。例如，统计两部剧的用户观看行为，用户A的观看向量为(0,1)，用户B为 (1,0);此时二者的余弦距离很大，而欧氏距离很小;我们分析两个用户对于不同视频的偏好，更关注相对差异，显然应当使用余弦距离。而当我们分析用户活跃 度，以登陆次数(单位:次)和平均观看时长(单位:分钟)作为特征时，余弦距离会 认为(1,10)、(10,100)两个用户距离很近;但显然这两个用户活跃度是有着极大差异的，此时我们更关注数值绝对差异，应当使用欧氏距离。

ab测试的陷阱？
  注意样本的独立性和采样方式的无偏性

AUC是指从一堆样本中随机抽一个，抽到正样本的概率比抽到负样本的概率大的可能性：https://www.douban.com/note/284051363/
Ks值：http://www.sohu.com/a/132667664_278472
混淆矩阵：http://blog.csdn.net/u010705209/article/details/53037481
  http://m.blog.csdn.net/dashenghuahua/article/details/53841630
  http://blog.csdn.net/xbmatrix/article/details/62056589





# 面试题目
<!-- 特征工程 -->
1. 特征变换或者说特征衍生、特征提取有用到哪些方法？
2. 哪些机器学习算法需要做归一化处理？为什么要做归一化？
3. 稀疏型数据更适合什么样的模型？
4. 有哪些文本表示模型？lda和词嵌入的区别？
5. 什么是组合特征?如何处理高维组合特征? 
6. 在图像分类任务中，训练数据不足会带来什么问题?如何缓解数据量不足带来的问题?
7. 为何需要特征选择？如何特征选择？
8. 为什么深度学习能在大规模数据下比传统机器学习算法能发挥更有效的作用？
<!-- 模型评估 -->
1. 验证集和测试集的作用分别是什么？
2. 怎么验证两个时间窗口下的样本分布不一致？
3. 诊断模型时如何判断过拟合、欠拟合？怎么解决，从多角度思考？
4. ROC曲线怎么画的，PR曲线是什么，哪个受不平衡样本影响？
5. AUC的统计学意义是什么，怎么计算、分布式下又如何计算？
6. 解释：期望泛化误差 = 偏差+方差+噪声？中偏差和方差的理解，以及这里的误差、偏差、方差是怎么定义的？
7. 余弦距离、K-L散度是否是一个严格意义上的距离？
8. ab测试时样本划分应该注意什么？
9. 解释召回率、精确率、准确率？
10. MLE（最大似然估计）和MAP（最大后验估计）的区别？




# 论文
Causality-based Feature Selection: Methods and Evaluations
An embedded feature selection method for imbalanced data classification
A two-layer feature selection method using Genetic Algorithm and Elastic Net Author links open overlay panel
Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review
kaushalshetty/FeatureSelectionGA
Alxe1/FeatureSelectionsAndExtractions
Jie-Yuan/FeatureSelector





