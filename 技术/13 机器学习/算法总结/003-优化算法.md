## 优化算法
优化算法总结：http://blog.csdn.net/u012759136/article/details/52302426

有监督学习的损失函数有哪些？

从损失函数中的经验风险和结构风险角度理解下 MLE（极大似然估计）和 MAP（最大后验概率）？

L1正则化相比L2正则化能使得模型参数具有稀疏解的原理？
  L1：稀疏，好处是特征选择、可解释性强；
  L2: 可以看成是权值的高斯先验；
    让求解变得稳定；
    数值上更容易求解；
    控制模型的复杂度，光滑性；复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。
    减小参数空间；参数空间越小，复杂度越低。 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。
  对比：3个角度：
      依据KKT条件可得带约束条件和带正则等价，观察解空间和等高线；
      拉普拉斯先验和高斯先验；
      函数叠加

训练数据量很大时，经典的梯度下降法存在什么问题？需要如何改进？
  随机梯度下降

牛顿法和拟牛顿法的理解？

随机梯度下降法的变种？随机梯度下降法的加速
  - SGD，学习过程可能会比较慢
  - Momentum，动量方法，加速学习
  - AdaGrad，AdaDelta，自适应学习率 
  - RMSProp，
  - Adam，AdaMax, Nadam
  - Ftrl，Nesterov 

启发式搜索算法有哪些？避免局部最优？

如何验证求目标函数梯度功能的正确性?

如何判断一个问题是不是凸优化问题，举例？
  凸优化（逻辑回归）、非凸优化（主成分分析、神经网络）
  计算目标函数的二阶Hessian矩阵来验证凸性；

拉格朗日乘子法：KKT条件？解的下界？对偶问题？


## 概率\统计\信息论\数学等
理解独立和相关？频率学派和贝叶斯学派？大数定律？贝叶斯公式，协方差矩阵？

极大似然思想？参数估计的置信区间、置信度的含义？假设检验的思想？
  目的：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
  原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。
  极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

互信息，交叉熵，条件熵，K-L散度？

协方差和相关性有什么区别？
  相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

采样多少合适？
  论样本与总体、置信度的关系。

给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？
  答：约有32%的数据将不受缺失值的影响。因为，由于数据分布在中位数附近，让我们先假设这是一个正态分布。我们知道，在一个正态分布中，约有68%的数据位于跟平均数（或众数、中位数）1个标准差范围内，那么剩下的约32%的数据是不受影响的。因此，约有32%的数据将不受缺失值的影响。

解释置信区间？
  置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%

优化算法：
  - adamW，Cocob优化，
  - 常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）
  - 遗传算法 随机搜寻 爬山算法 Nelder-Mead 算法


# 面试题目
<!-- 优化算法 -->
1. 从损失函数中的经验风险和结构风险角度理解下 MLE（极大似然估计）和 MAP（最大后验概率）？
1. L1正则化相比L2正则化能使得模型参数具有稀疏解的原理？
2. 牛顿法和拟牛顿法的理解？
3. 随机梯度下降法的变种？随机梯度下降法的加速
4. 如何判断一个问题是不是凸优化问题，举例？
5. 拉格朗日乘子法：KKT条件？解的下界？对偶问题？
<!-- 概率统计 -->
1. 极大似然思想？参数估计的置信区间、置信度的含义？假设检验的思想？
2. 互信息，交叉熵，条件熵，K-L散度？



