- 贝叶斯模型：
    朴素贝叶斯（NB），伯努利、高斯、多项式
    半朴素贝叶斯
    贝叶斯信念网络，动态信念网络
    平均单依赖估计（Averaged One-Dependence Estimators， AODE，平均-依赖性估计AODE）
    贝叶斯网络
    贝叶斯ARD回归，贝叶斯岭回归
    贝叶斯学习

1	核心思想
利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；
假设：属性类条件独立；
2	模型
生成模型，计算条件概率
3	策略
期望风险最小化
4	算法
由训练数据学得联合概率，再得到后验概率；
频率估计概率算先验概率；极大似然估计算类条件概率。
贝叶斯估计代替极大似然估计防止出现概率为0的情况；
5	模型特点应用场景
5.1	优点
对小规模数据表现良好；
  适合多分类任务；
  适合增量学习；
  能给出类别猜测结果的概率值；
  对缺失数据不敏感；
5.2	缺点
  对输入数据的表达形式敏感；
  不能学习特征之间的相互作用；
  模型简单引起偏差高方差低；
  严重依赖假设的概率分布形式；
5.3	场景
文档分类；病人分类；垃圾邮件过滤；
  要求不同维度之间相关性较小；适用标称数据；
6	相关模型
高斯贝叶斯，多项贝叶斯，伯努利贝叶斯，词袋模型和词集模型；
7	补充
1)	朴素贝叶斯介绍？为什么朴素贝叶斯如此“朴素”？  
利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；由训练数据学得联合概率，再得到后验概率；它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
2)	LR NB的区别？
1.朴素贝叶斯分类器将会比判别模型，譬如逻辑回归收敛得更快，因此你只需要更少的训练数据。
2.其主要缺点是它学习不了特征间的交互关系
3)	为什么说朴素贝叶斯是高偏差低方差？
它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差
4)	解释置信区间？
置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%

朴素贝叶斯介绍？为什么朴素贝叶斯如此“朴素”？  
利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；由训练数据学得联合概率，再得到后验概率；它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
LR NB的区别？
1.朴素贝叶斯分类器将会比判别模型，譬如逻辑回归收敛得更快，因此你只需要更少的训练数据。
2.其主要缺点是它学习不了特征间的交互关系
为什么说朴素贝叶斯是高偏差低方差？
它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差
解释置信区间？
置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%


数值型；标称型
文档分类；病人分类；垃圾邮件过滤；
  要求不同维度之间相关性较小；适用标称数据 

高斯贝叶斯，多项贝叶斯，伯努利贝叶斯，词袋模型和词集模型；
缺点
易欠拟合 
优点
优点
对小规模数据表现良好；
  适合多分类任务；
  适合增量学习；
  能给出类别猜测结果的概率值；
  对缺失数据不敏感；
缺点
  对输入数据的表达形式敏感；
  不能学习特征之间的相互作用；
  模型简单引起偏差高方差低；
  严重依赖假设的概率分布形式；
由训练数据学得联合概率，再得到后验概率；
频率估计概率算先验概率；极大似然估计算类条件概率。
贝叶斯估计代替极大似然估计防止出现概率为0的情况；
期望风险最小化
生成模型，计算条件概率
利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；
假设：属性类条件独立；

62. 朴素贝叶斯介绍？为什么朴素贝叶斯如此“朴素”？  利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；由训练数据学得联合概率，再得到后验概率；它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
63. LR NB的区别？1.朴素贝叶斯分类器将会比判别模型，譬如逻辑回归收敛得更快，因此你只需要更少的训练数据。2.其主要缺点是它学习不了特征间的交互关系
64. 贝叶斯分类器：依据条件分布将样本分到条件概率最大的类中。

65. 分类器：决策函数形式或条件概率形式
66. 为什么说朴素贝叶斯是高偏差低方差？它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差