- 聚类算法：
   - 基于划分（
       k-means，k-means++，k-modes，FK-Means，k-medoids，
       学习向量量化（LVQ），
       高斯混合模型（GMM），
       贝叶斯高斯混合模型， 
       K-Pototypes，CLARANS）
   - 基于密度（DBSCAN）
       局部密度聚类，
       OPTICS，
       DENCLUE，
       CURE
   - 基于层次（AGNES），
       概念聚类，
       単连锁聚类，
       BIRCH，
       DIANA，
       WARD聚类，
       ROCK
   - 基于网格（STING），
       CLIQUE(基于密度和基于网格)，
       WaveCluster
   - 基于模型（EM），
       SOM，
       CobWeb，
   - 图(网络)聚类
   - 高维数据聚类（双聚类，谱聚类），
       FC，
       SVD聚类，
       K-medians，
       mean-shift，
       Canopy，
       AP算法，
       Agglomerative，
       聚类集成
   - 具有约束的聚类  


核心思想
目的：发现数据内在的分布结构；其它任务的前驱过程
簇内的相似度高、簇间相似度低
性能度量：外部指标和内部指标

k-means
特点：
  O(nkt)，算法简单快捷
  密集，球状
  Lloyd’s or Elkan’s algorithm
缺点：
  噪声数据
  k的确定
  非凸面形情形
  初值确定
  对类别属性不适用
选择批次距离尽可能远的K个点；首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个
KMeans初始类簇中心点的选取?k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。
   1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
   2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
   3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
   4. 重复2和3直到k个聚类中心被选出来
   5. 利用这k个初始的聚类中心来运行标准的k-means算法。

GMM
特点：
  概率模型表达聚类原型，样本的生成由高斯混合分布给出。
  样本i由第j个高斯混合成分生成的后验概率\gamma_ji，簇划分由原型对应的后验概率确定
  模型参数的确定：EM算法进行迭代优化
  E：根据当前参数计算每个样本属于每个高斯成分的后验概率
  M：根据极大似然和拉格朗日乘子法更新模型参数

LVQ
特点：
  SOM是基于无标记样本的聚类；而此算法是SOM基于监督信息的扩展

DBSCAN
特点：
  可识别噪声和异常点
  先找出核心对象集合，从中选一个作为种子，由其密度可达的样本生成簇；
  继续从剩余的集合中选一个种子
  ball_tree’, ‘kd_tree’, ‘brute’

AGNES
特点：
  自底向上
  缺点：


SVD和SVD++:
  降维，聚类隐语义索引、信息检索；图像压缩，推荐系统http://blog.csdn.net/u011412768/article/details/52972081


采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。


# 面试题目
1. k-means的k个点初始化过程？
2. 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
