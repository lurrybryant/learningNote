  参照百面机器学习目录归纳

## 特征工程
- 机器学习与数据挖掘的区别？
  1. 机器学习是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。
  2. 数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的模式。在这个过程中应用了机器学习算法，另外还用到数据库，数据仓库，统计学，信息检索。

- 从数据分布的角度来讲，做监督学习的基本假设是什么？三要素是什么？从这三个角度思考在数据输入固定的情况下怎么调优模型？
  1. 同类数据具有一定的统计规律性（特征与标记具有联合概率分布），且满足独立同分布条件
  2. 模型：模型的假设空间，概率模型（条件概率分布）和非概率模型（决策函数）
     策略：模型选择的准则
     算法：模型学习的算法，解析解或数值计算方法最优化
  3. 选择学习模型：
        训练时间；预测时间；内存消耗；理解和调参；
        线性分类器速度快、编程方便，但是可能拟合效果不会很好；
        非线性分类器编程复杂，但是特征比数据量还大时效果拟合能力强；
        模型融合；
     选择学习策略：选择合适的正则化；正则化系数c；损失函数，给不同权值；
     实现求解最优模型的算法：最优化算法选择；收敛的阈值或迭代次数；

- 为什么深度学习能在大规模数据下比传统机器学习算法能发挥更有效的作用？
  答：？

- 为什么说数据本身决定了模型的上限？
  1. 数量、质量（冗余、无关、噪声）、代表性

- 特征变换或者说特征衍生有用到哪些方法？
  1. 基于函数运算、
  2. 组合特征（基于决策树）、
  3. 离散化分箱、
  4. 类别型变量编码（序号，独热，二进制）、
  5. 聚类、PCA降维

- 为何需要特征选择？如何特征选择？
  1. 去冗余、去无关、增强解释性、降低维度减轻学习负担、减少过拟合
  2. 理解业务，过滤式、包裹式、嵌入式、基于模型、基于稳定性

- 特征提取的方法？
  1. PCA和LDA降维：PCA是为了让映射后的样本具有最大的发散性（无监督）；LDA是为了让映射后的样本有最好的分类
  2. 神经网络

- 哪些机器学习算法需要做归一化处理？为什么要做归一化？
  1. 逻辑回归等模型先验假设数据服从正态分布。
  2. 避免受尺度较大的特征影响。
     加快梯度下降求最优解的速度

- 稀疏型数据更适合什么样的模型？
  1. Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。
  2. GBDT更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低纬稠密特征，进一步引入了语意信息有利于特征的表达。

- 有哪些文本表示模型？
  词袋模型、TF-IDF，主题模型，词嵌入模型，Word2Vec（cbow, skip-gram, LDA）

## 模型评估
- 验证集和测试集的作用是什么？

- 交叉验证和自主采样？

- 诊断模型时如何判断过拟合、欠拟合？怎么解决？
 1. 数据两个维度、
 2. 模型、
 3. 训练过程、
 欠拟合：
  增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
  增加模型复杂度，尝试非线性模型，比如核SVM 等模型;
  增加训练轮数
 过拟合：
  增加数据量，正确抽样，减少噪声干扰（对于按区间离散化的特征，增大划分的区间），数据扩增（假设便是，训练数据与将来的数据是独立同分布的，扩增方法：从数据源头采集更多数据；复制原有数据并加上随机噪声；重采样；根据当前数据集估计数据分布参数，使用该分布产生更多数据等）；对样本进行降维；
  降低模型复杂度，参数过多，模型复杂度比真模型高，选择简单的模型；集成学习；
  控制训练轮数，比如最优化求解时，收敛之前停止迭代；决策树剪枝；神经网络在模型对训练数据集迭代收敛之前停止迭代来防止过拟合；集成学习计算验证集的精确度，不再提高就停止训练）；

- 精确率和召回率、混淆矩阵、roc曲线怎么画的（当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，大数据下怎么算auc）、PR曲线是什么
 1. 真实为正例中预测为正例的比例，真实为反例中预测为正例的比例

- 期望泛化误差 = 偏差+方差+噪声？
 1. 泛化性能是由学习算法的能力、数据的充分性、学习任务的本身难度决定；

- 生成模型和判别模型的特点？
 1. 数据集小：选高偏差低方差的简单分类器，生成模型；
 2. 数据集大：选低偏差高方差的复杂分类器，判别模型；

- 余弦距离、K-L散度是否是一个严格意义上的距离？


## 优化算法
- 从损失函数中的经验风险和结构风险角度理解下 MLE（极大似然估计）和 MAP（最大后验概率）？

- L1正则化相比L2正则化能使得模型参数具有稀疏解的原理？
 1. L1：稀疏，好处是特征选择、可解释性强；
 2. L2: 可以看成是权值的高斯先验；
    让求解变得稳定；
    数值上更容易求解；
    控制模型的复杂度，光滑性；复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。
    减小参数空间；参数空间越小，复杂度越低。 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。
 3. 对比：3个角度：
      依据KKT条件可得带约束条件和带正则等价，观察解空间和等高线；
      拉普拉斯先验和贝叶斯先验；
      函数叠加

- 训练数据量很大时，经典的梯度下降法存在什么问题？需要如何改进？
 1. 随机梯度下降

- 牛顿法和拟牛顿法的理解？

- 随机梯度下降法的变种？
  - SGD，学习过程可能会比较慢
  - Momentum，动量方法，加速学习
  - AdaGrad，AdaDelta，自适应学习率 
  - RMSProp，
  - Adam，AdaMax, Nadam
  - Ftrl，Nesterov 

- 启发式搜索算法有哪些？避免局部最优？

- 拉格朗日乘子法：KKT条件？解的下界？对偶问题？

- 如何判断一个问题是不是凸优化问题，举例？
 1. 凸优化（逻辑回归）、非凸优化（主成分分析、神经网络）

## 采样
- 数据不平衡问题对模型有什么影响？数据偏斜不能过于严重（unbalanced），不同类别的数据数量不要有数个数量级的差距，数据不平衡问题如何解决？
  1. 影响：
  2. 解决：
       阈值移动，
       上采样(smote，有放回)，
       下采样（EasyEnsemble），
       代价敏感学习（修改权重）

- 如果数据量太大怎么处理？

## 概率\统计\信息论等
- 理解独立和相关？频率学派和贝叶斯学派？大数定律？贝叶斯公式，协方差矩阵？
- 极大似然思想？参数估计的置信区间、置信度的含义？假设检验的思想？
 1. 目的：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
    原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。
    极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

- 互信息，交叉熵，条件熵，K-L散度？







学习资料：
https://woaielf.github.io/2016/09/11/data-science/
https://woaielf.github.io/2017/04/06/dm-6/
https://woaielf.github.io/page4/
使用sklearn做单机特征工程：
http://m.blog.csdn.net/MrLevo520/article/details/78085650
维基百科：https://en.wikipedia.org/wiki/Gradient_boosting#Shrinkage
归一化：http://blog.csdn.net/xbmatrix/article/details/56695825
深度学习：https://www.leiphone.com/news/201608/7lwVZCXnScbQb6cJ.html
深度学习tf：https://www.zhihu.com/question/41667903?from=profile_question_card
安装xgboost https://www.cnblogs.com/haobang008/p/5907854.html

面试题目：
http://m.blog.csdn.net/dashenghuahua/article/details/53841630
http://blog.csdn.net/xbmatrix/article/details/62056589
——————————————————————————————————
1.  统计机器学习三要素？模型，策略，算法
2.  常见的生成式模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等；
3.  深度学习：DNN, CNN, RNN, LSTM, GAN
4.  聚类：k-means, 
5.  降维：SVD SVD++
6.  机器学习与数据挖掘的区别？机器学习是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的模式。在这个过程中应用了机器学习算法。
7.  线性分类器与非线性分类器的区别以及优劣？线性分类器速度快、编程方便，但是可能拟合效果不会很好；非线性分类器编程复杂，但是效果拟合能力强特征比数据量还大时，
8.  数据维度高时选择什么样的分类器？线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分
9.  下面是吴恩达的见解：1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况
10. ill-condition病态问题？训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）
11. 如何模型调优？选择合适的正则化；正则化系数c；收敛的阈值或迭代次数；调整损失函数，给不同权值；bagging模型融合；最优化算法选择；
12. 模型选择要考虑的问题？模型效果；训练时间；预测时间；内存消耗；理解和调参；
13. 常见的七个损失函数是？http://blog.csdn.net/heyongluoyao8/article/details/52462400
14. 正则化有哪些？都有什么好处？
15. L1求解？最小角回归算法：LARS算法
16. 如何评估模型性能好坏？均方误差，准确率，F1为精确率和召回率的调和平均，AUC（真实为正例中预测为正例的比例，真实为反例中预测为正例的比例）  http://www.jianshu.com/p/6ffa3df3ec86
17. 因为当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。
18. 基于回归任务的期望泛化误差？期望泛化误差=偏差+方差+噪声；偏差描述算法本身的拟合能力，方差刻画数据扰动造成的影响，噪声刻画问题本身的难度。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。
19. 解决bias和Variance问题的方法：交叉验证。High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征。High Variance解决方案：bagging、简化模型、降维
20. 你意识到你的模型受到低偏差和高方差问题的困扰。那么，应该使用哪种算法来解决问题呢？答：可以使用bagging算法（如随机森林）。因为，低偏差意味着模型的预测值接近实际值，换句话说，该模型有足够的灵活性，以模仿训练数据的分布。这样貌似很好，但是别忘了，一个灵活的模型没有泛化能力，意味着当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。
21. 另外，为了应对大方差，我们可以：1.使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。2.使用可变重要性图表中的前n个特征。可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。
22. 交叉验证的思想？评估模型性能方法？简单交叉；S折交叉验证；留一法交叉验证；自助采样（有放回）：
23. 你会在时间序列数据集上使用什么交叉验证技术？是用k倍或LOOCV？5倍正向链接策略
24. 过拟合？如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。
25. 越小的参数说明模型越简单？过拟合的，拟合会经过曲面的每个点，也就是说在较小的区间里面可能会有较大的曲率，这里的导数就是很大，线性模型里面的权值就是导数，所以越小的参数说明模型越简单。
26. 如何判断过拟合？训练集拟合得很好但验证集上预测效果不好
27. 过拟合产生的原因？1. 样本数据的问题。样本数量太少，抽样方法错误，抽出的样本数据不能有效足够代表业务逻辑或业务场景。比如样本符合正态分布，却按均分分布抽样，或者样本数据不能代表整体数据的分布，样本里的噪音数据干扰过大2. 模型问题。模型复杂度高 、参数太多，决策树模型没有剪枝，权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.
28. 处理过拟合？1. 样本数据方面。增加样本数量，数据扩增（假设便是，训练数据与将来的数据是独立同分布的，扩增方法：从数据源头采集更多数据；复制原有数据并加上随机噪声；重采样；根据当前数据集估计数据分布参数，使用该分布产生更多数据等）；对样本进行降维；添加验证数据；抽样方法要符合业务场景；清洗噪声数据；对于按区间离散化的特征，增大划分的区间；2. 模型或训练问题。控制模型复杂度，优先选择简单的模型；或者用模型融合技术；利用先验知识；添加正则项，L1正则更加容易产生稀疏解、L2正则倾向于让参数w趋向于0；交叉验证；不要过度训练，训练早停（比如最优化求解时，收敛之前停止迭代；决策树剪枝；神经网络在模型对训练数据集迭代收敛之前停止迭代来防止过拟合；集成学习计算验证集的精确度，不再提高就停止训练）；Dropout（修改ANN中隐藏层的神经元个数来防止ANN的过拟合）
29. 欠拟合怎么办？1.增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;2.尝试非线性模型，比如核SVM 等模型;3.增加训练轮数
30. 常用数值优化方法？mini-batch （部分增量更新）与 full-batch（全增量更新）牛顿法，拟牛顿法，在斜率(方向导数)大的地方，使用小学习率，在斜率(方向导数)小的地方，使用大学习率http://blog.csdn.net/u012328159/article/details/51613262
31. 数据不平衡问题？阈值移动，上采样(smote，有放回)，下采样（easy ensemble），代价敏感学习（修改权重）
32. 多分类问题？ovo（n(n-1)个分类器） ovr（n个分类器） mvm（ECOC）;
33. 数据预处理之数据光滑技术？（分箱 回归 离群点分析 聚集）
34. 数据预处理之缺失值？ 1. 缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。2.缺省值适中，单独当做一类；3. 缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理，比如，用全局常量填充；使用与给定元组同一类的所有样本的属性均值或中位数填充；使用最可能的值填充；用随机森林等算法预测填充；在保证原有数据样本分布不变情况下进行随机填充；拉格朗日插值法
35. 数据预处理之数据规约？聚集删除冗余特征；聚类；维归约（数据压缩 小波变换 属性子集选择 属性构造 PCA数值归约）；参数（回归 对数线性模型）和非参数（直方图 距离 抽样 数据立方体聚集）
36. 数据预处理之数据变换？压缩数据：分有损和无损压缩；规范化；离散化（按频次切分分箱bin，单位向量化；onehot编码或叫哑编码；映射到高维空间，降低过拟合）；由标称数据产生概念分层；数据光滑；聚集，组合特征，属性构造（基于多项式，基于指数，基于对数）；
37. 数据预处理之数据冗余及相关分析？卡方检验，相关系数，协方差
38. 数据预处理之元组重复检测，数据冲突检测与处理？
39. 标准化与归一化的区别？简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：
40. 数据预处理之归一化(归一到-1,1区间内)标准化（标准正态分布，Z-score）区间缩放（MinMax到0,1区间内）处理好处？ 1)归一化化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。归一化后加快了梯度下降求最优解的速度。等高线变得显得圆滑，在梯度下降进行求解时能较快的收敛。如果不做归一化，梯度下降过程容易走之字，很难收敛甚至不能收敛2）把有量纲表达式变为无量纲表达式, 有可能提高精度。一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）3) 逻辑回归等模型先验假设数据服从正态分布。
41. 哪些机器学习算法不需要做归一化处理？概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、gbdt、xgboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
42. 特征向量的归一化方法？线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)对数函数转换，表达式如下：y=log10 (x)反余切函数转换 ，表达式如下：y=arctan(x)*2/PI减去均值，乘以方差：y=(x-means)/ variance
43. 数据预处理之校准方法？普拉特校准，2. 保序回归
44. 特征工程之对定量特征二值化，确定阈值
45. 特征工程之特征选择？过滤（对发散性（方差选择）和相关性评分（皮尔逊相关系数和距离相关系数，卡方检验，互信息和最大信息系数）），包装（根据预测效果评分，递归消除特征），嵌入（基于惩罚项，基于树模型），基于模型（随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题））稳定性选择（是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。）
46. 为何进行特征选择？特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解
47. 特征工程之特征提取（降维），为什么要特征提取？特征太多，那么会产生不相关特征引入、过度拟合等问题，所以要特征提取。PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。
48. 给你一个有1000列和1百万行的训练数据集，这个数据集是基于分类问题的。经理要求你来降低该数据集的维度以减少模型计算时间，但你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设。）答：你的面试官应该非常了解很难在有限的内存上处理高维的数据。以下是你可以使用的处理方法： 1.由于我们的RAM很小，首先要关闭机器上正在运行的其他程序，包括网页浏览器等，以确保大部分内存可以使用。2.我们可以随机采样数据集。这意味着，我们可以创建一个较小的数据集，比如有1000个变量和30万行，然后做计算。3.为了降低维度，我们可以把数值变量和分类变量分开，同时删掉相关联的变量。对于数值变量，我们将使用相关性分析；对于分类变量，我们可以用卡方检验。4.另外，我们还可以使用PCA（主成分分析），并挑选可以解释在数据集中有最大偏差的成分5.利用在线学习算法，如VowpalWabbit（在Python中可用）是一个不错的选择。　6.利用Stochastic GradientDescent（随机梯度下降法）建立线性模型也很有帮助。7.我们也可以用我们对业务的理解来估计各预测变量对响应变量的影响的大小。但是，这是一个主观的方法，如果没有找出有用的预测变量可能会导致信息的显著丢失。
49. 统计：协方差和相关性有什么区别？答：相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。
50. 采样多少合适？论样本与总体、置信度的关系。
51. 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？答：约有32%的数据将不受缺失值的影响。因为，由于数据分布在中位数附近，让我们先假设这是一个正态分布。我们知道，在一个正态分布中，约有68%的数据位于跟平均数（或众数、中位数）1个标准差范围内，那么剩下的约32%的数据是不受影响的。因此，约有32%的数据将不受缺失值的影响。
52. 线性回归中线性相关的两个特征对线性模型的影响？ill-condition
53. 回归算法：线性回归，广义线性回归，多项式，岭回归，拉索回归，弹性网回归，逐步回归，局部加权线性回归（LWLR），SVR，RVM，KNN 回归，基于决策树，神经网络
54. 线性模型低方差高偏差，kNN高方差低偏差(kNN改进：核方法用权重；高维空间用更合适的距离度量)
55. 期望平方损失推出的预测是在一个点处的条件期望，kNN的预测是在一个区域内的期望。
56. kNN和最小二乘法都是通过平均来近似条件期望；最小二乘假设决策函数可以用一个全局线性函数近似，而kNN假设用一个局部常数函数来近似；
57. kNN在高维中样本稀疏问题；维度灾难，采样密度问题；
58. MSE = 方差+偏差的平方
59. 逻辑回归，损失函数，以及如何求参数？LR模型是输出Y=1的对数几率是输入x的线性表示的模型，进而得到输出Y=1的概率是关于输入的线性表示的sigmoid函数；损失函数是对数似然函数的相反数，利用数值优化算法求解最优参数值，最后学得逻辑回归模型为两个概率；推广位多项逻辑回归模型用于多分类。 
60. 我知道校正R2或者F值是用来评估线性回归模型的。那用什么来评估逻辑回归模型？　1.由于逻辑回归是用来预测概率的，我们可以用AUC-ROC曲线以及混淆矩阵来确定其性能。2.此外，在逻辑回归中类似于校正R2的指标是AIC。AIC是对模型系数数量惩罚模型的拟合度量。因此，我们更偏爱有最小AIC的模型。3.空偏差指的是只有截距项的模型预测的响应。数值越低，模型越好。残余偏差表示由添加自变量的模型预测的响应。数值越低，模型越好。
61. 极大似然估计的思想？最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。
62. 朴素贝叶斯介绍？为什么朴素贝叶斯如此“朴素”？  利用先验概率得到后验概率，由最小化期望风险得到后验概率最大化，从而输出使得后验概率最大化的值；由训练数据学得联合概率，再得到后验概率；它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
63. LR NB的区别？1.朴素贝叶斯分类器将会比判别模型，譬如逻辑回归收敛得更快，因此你只需要更少的训练数据。2.其主要缺点是它学习不了特征间的交互关系
64. 贝叶斯分类器：依据条件分布将样本分到条件概率最大的类中。
65. 分类器：决策函数形式或条件概率形式
66. 为什么说朴素贝叶斯是高偏差低方差？它简单的假设了各个特征之间是无关的，是一个被严重简化了的模型。所以，对于这样一个简单模型，大部分场合都会bias部分大于variance部分，也就是高偏差，低方差
67. 解释置信区间？置信区间不能用贝叶斯学派的概率来描述，它属于频率学派的范畴。真值要么在，要么不在。由于在频率学派当中，真值是一个常数，而非随机变量（后者是贝叶斯学派），所以我们不对真值做概率描述。比如，95%置信区间，并不是真值在这个区间内的概率是95%，而应该为100次随机抽样中构造的100个区间如果95次包含了参数真值，那么置信度为95%
68. 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？答：我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。
69. KNN三要素？K值选择，距离度量，决策规则
70. kNN时间复杂度？O(n)
71. 优化KNN?使用kd树或者ball tree(这个树不懂),将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可
72. SVM讲解？它是定义在特征空间上使间隔最大的线性分类器；考虑硬间隔最大化，优化问题为函数间隔不小于1时最大化支持向量到分离平面的几何间隔；拉格朗日对偶性，原始问题为极小极大L函数转换为对偶问题极大极小；用SMO算法求解该对偶问题；根据KKT条件算出原问题的解；最后得到决策函数
73. 几何间隔?实例点到超平面的带符号的距离
74. 硬间隔最大化？意味着以充分大的确信度对训练数据进行分类
75. 软间隔最大化？加入了松弛变量
76. SVM核函数理解；常用的核函数；为什么KKT条件可以求出参数?核函数将输入转换为特征空间中特征向量的内积；线性核函数，多项式核函数，高斯核函数，拉普拉斯核函数，Sigmoid核函数;KKT条件保证转化为对偶问题的求解的合理性
77. 解释对偶的概念?一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。
78. SVM与LR的区别？1.SVM分类器只由很少的训练样本确定；
79. http://blog.csdn.net/szlcw1/article/details/52259668数据挖掘（机器学习）面试--SVM面试常考问题
80. SVM、LR、决策树的对比？模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核
81. 决策树的算法包含？特征选择，决策树生成，剪枝（模型选择，避免过拟合，根据最小损失函数原则剪枝）。
82. 决策树属性选择的指标的含义？信息增益（数据集中类与属性A的互信息，表示由于特征A使得对数据集D的分类不确定性减少的程度，偏向取值较多的属性），信息增益率（信息增益与数据关于特征A的熵之比，先选出信息增益高于平均水平的属性），gini指数（表示经过属性A分割后数据集的不确定性或均匀度）
83. 决策树如何回归？最小二乘回归树算法：平方误差最小准则，每个内部节点构建时选择最优切分变量j和切分点S
84. CART剪枝算法用到了后剪枝和验证集（交叉验证）
85. Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？答：最根本的区别是，随机森林算法使用bagging技术做出预测；而GBM是采用boosting技术做预测的。1.在bagging技术中，数据集用随机采样的方法被划分成n个样本。然后，使用单一的学习算法，在所有样本上建模。2.接着利用投票或者求平均来组合所得到的预测。3.bagging是平行进行的，而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在后续一轮中得到校正。这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。4.随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。在另一方面，GBM提高了精度，同时减少了模型的偏差和方差
86. 随机森林如何评估特征重要性（http://charleshm.github.io/2016/03/Random-Forest-Tricks/）?1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。
87. 花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM（Gradient Boosted Models），想着boosting算法会展现“魔力”。不幸的是，没有一个模型比基准模型表现得更好。最后，你决定将这些模型结合到一起。尽管众所周知，结合模型通常精度高，但你就很不幸运。你到底错在哪里？　答：据我们所知，组合的学习模型是基于合并弱的学习模型来创造一个强大的学习模型的想法。但是，只有当各模型之间没有相关性的时候组合起来后才比较强大。由于我们已经试了5个GBM也没有提高精度，表明这些模型是相关的。具有相关性的模型的问题是，所有的模型提供相同的信息。例如：如果模型1把User1122归类为1，模型2和模型3很有可能会做同样的分类，即使它的实际值应该是0，因此，只有弱相关的模型结合起来才会表现更好。
88. 简述Adaboost算法过程？模型为加法模型，策略是指数损失函数（训练误差以指数速率下降），算法为前向分步算法（训练数据初始权重为1/n，训练出一个分类器并得到误差率和该分类器的权重，接着更新训练数据的权值分布或者重采样）由前向分步算法思想，将基本分类器设为决策树（二叉分类树和二叉回归树），回归问题损失函数为平方误差损失，可以转化为拟合残差或损失函数的负梯度在当前模型的值（GDBT），分类问题为指数损失。
89. 讲一讲GBDT，问为什么对损失函数求负梯度可以达到拟合残差的作用？泰勒一阶展开解释。梯度提升法。
90. 如何控制GBDT过拟合？每棵树叶子节点个数在4—8之间；通过验证集来确定树的棵数M的大小（正则化）；通过缩减来正则化（加入学习率v）；随机梯度提升（每个基学习器的采样数为0.5），减少了方差（过拟合）；叶子节点中包含的样本最少个数；加入决策树的惩罚项用于后剪枝（结构风险）（叶子节点个数或者关于叶子节点数的L2惩罚项）
91. 简述Bagging算法过程？随机森林：属性选择扰动；样本扰动（自助采样，有放回），提高基学习器之间的差异性进而提高泛化性能。降低方差，对神经网络、决策树这样的易受样本扰动的学习器上效用更为明显
92. 如何增加基学习器的多样性？数据样本扰动，输入属性扰动，输出表示扰动，算法参数扰动
93. 简述结合策略stacking：初级学习器的输出作为次级学习器的输入
94. xgboost怎么给特征评分？在训练的过程中，通过结构分数的增益情况选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。
95. 聚类算法有哪些；
96. Gaussian clusters?
97. K-means时间复杂度为O(knt) 
98. 选择批次距离尽可能远的K个点；首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个
99. KMeans初始类簇中心点的选取?k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大4. 重复2和3直到k个聚类中心被选出来5. 利用这k个初始的聚类中心来运行标准的k-means算法
100.  采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦
101.  SVD和SVD++:降维，聚类隐形语义索引、信息检索； 图像压缩，推荐系统http://blog.csdn.net/u011412768/article/details/52972081
102.  讲一讲DNN
103.  CNN
104.  LSTM
105.  讲一讲sigmoid和reLU的区别，以及各自应该用在什么情况下
106.  讲一讲sigmoid和softmax的区别
107.  讲高维运动数据处理为什么要用谱聚类，不用别的聚类方法
108.  讲一讲LDA，模拟过程，以及训练和推断过程
109.  情景题
a 现在有一堆车牌信息（例如 京A 12345），然后是其中有百分之二十的车牌有登记时间，现在要把剩下百分之八十的车牌登记上时间，问怎么做
b 现在有人工标记的一些图片，分为五个等级（非常好，好，中，差，非常差），现在需要来预测新的图片（softmax），可是发现预测出的图片有些问题即当非常好的概率为最大时，有时候差的概率是次大的，这是不符合预期的，问应该怎么做 

1.  连续监督学习有什么不同方法？滑动窗口方法2. 复发性推拉窗3. 隐藏马尔科夫模型4. 最大熵马尔科夫模型5. 条件随机域6. 图变换网络。
2.  什么是PAC学习？可能近似正确模型 (PAC) 学习是一个已经被引入到分析学习算法和统计效率的学习框架
3.  有哪些不同的类别可以分为序列学习过程？序列预测2. 序列生成3. 序列识别4. 顺序决定.
