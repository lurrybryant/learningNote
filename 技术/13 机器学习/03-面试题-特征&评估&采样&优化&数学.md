  参照百面机器学习目录归纳
  面试题目：
http://m.blog.csdn.net/dashenghuahua/article/details/53841630
http://blog.csdn.net/xbmatrix/article/details/62056589
——————————————————————————————————

## 特征工程

- 机器学习与数据挖掘的区别？
  1. 机器学习是指在没有明确的程序指令的情况下，给予计算机学习能力，使它能自主的学习、设计和扩展相关算法。
  2. 数据挖掘则是一种从非结构化数据里面提取知识或者未知的、人们感兴趣的模式。在这个过程中应用了机器学习算法，另外还用到数据库，数据仓库，统计学，信息检索。

- 从数据分布的角度来讲，做监督学习的基本假设是什么？三要素是什么？从这三个角度思考在数据输入固定的情况下怎么调优模型？
  1. 同类数据具有一定的统计规律性（特征与标记具有联合概率分布），且满足独立同分布条件
  2. 模型：模型的假设空间，概率模型（条件概率分布）和非概率模型（决策函数）
     策略：模型选择的准则
     算法：模型学习的算法，解析解或数值计算方法最优化
  3. 选择学习模型：
        训练时间；预测时间；内存消耗；理解和调参；
        线性分类器速度快、编程方便，但是可能拟合效果不会很好；
        非线性分类器编程复杂，但是特征比数据量还大时效果拟合能力强；
        模型融合；
     选择学习策略：选择合适的正则化；正则化系数c；损失函数，给不同权值；
     实现求解最优模型的算法：最优化算法选择；收敛的阈值或迭代次数；

- 为什么深度学习能在大规模数据下比传统机器学习算法能发挥更有效的作用？
  答：？

- 为什么说数据本身决定了模型的上限？
  1. 数量、质量（冗余、无关、噪声）、代表性

- 特征变换或者说特征衍生有用到哪些方法？
  1. 基于函数运算、
  2. 组合特征（基于决策树）、
  3. 离散化分箱、
  4. 类别型变量编码（序号，独热，二进制）、
  5. 聚类、PCA降维

- 为何需要特征选择？如何特征选择？
  1. 去冗余、去无关、增强解释性、降低维度减轻学习负担、减少过拟合
  2. 理解业务，过滤式、包裹式、嵌入式、基于模型、基于稳定性

- 特征提取的方法？
  1. PCA和LDA降维：PCA是为了让映射后的样本具有最大的发散性（无监督）；LDA是为了让映射后的样本有最好的分类
  2. 神经网络

- 哪些机器学习算法需要做归一化处理？为什么要做归一化？
  1. 逻辑回归等模型先验假设数据服从正态分布。
  2. 避免受尺度较大的特征影响。
     加快梯度下降求最优解的速度

- 稀疏型数据更适合什么样的模型？
  1. Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。
  2. GBDT更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低纬稠密特征，进一步引入了语意信息有利于特征的表达。

- 有哪些文本表示模型？
  词袋模型、TF-IDF，主题模型，词嵌入模型，Word2Vec（cbow, skip-gram, LDA）

- 数据维度高时选择什么样的分类器？
  线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分
  下面是吴恩达的见解：
    1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 
    2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+GaussianKernel 
    3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

- ill-condition病态问题？
  训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）

- L1求解？最小角回归算法：LARS算法

## 模型评估

- 验证集和测试集的作用是什么？

- 交叉验证和自主采样？

- 诊断模型时如何判断过拟合、欠拟合？怎么解决？
 1. 数据两个维度、
 2. 模型、
 3. 训练过程、
 欠拟合：
  增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
  增加模型复杂度，尝试非线性模型，比如核SVM 等模型;
  增加训练轮数
 过拟合：
  增加数据量，正确抽样，减少噪声干扰（对于按区间离散化的特征，增大划分的区间），数据扩增（假设便是，训练数据与将来的数据是独立同分布的，扩增方法：从数据源头采集更多数据；复制原有数据并加上随机噪声；重采样；根据当前数据集估计数据分布参数，使用该分布产生更多数据等）；对样本进行降维；
  降低模型复杂度，参数过多，模型复杂度比真模型高，选择简单的模型；集成学习；
  控制训练轮数，比如最优化求解时，收敛之前停止迭代；决策树剪枝；神经网络在模型对训练数据集迭代收敛之前停止迭代来防止过拟合；集成学习计算验证集的精确度，不再提高就停止训练）；

- 精确率和召回率、混淆矩阵、roc曲线怎么画的（当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动，大数据下怎么算auc？）、PR曲线是什么
 1. 真实为正例中预测为正例的比例，真实为反例中预测为正例的比例

- 期望泛化误差 = 偏差+方差+噪声？
 1. 泛化性能是由学习算法的能力、数据的充分性、学习任务的本身难度决定；

- 生成模型和判别模型的特点？
 1. 数据集小：选高偏差低方差的简单分类器，生成模型；
 2. 数据集大：选低偏差高方差的复杂分类器，判别模型；

- 余弦距离、K-L散度是否是一个严格意义上的距离？


## 优化算法

- 从损失函数中的经验风险和结构风险角度理解下 MLE（极大似然估计）和 MAP（最大后验概率）？

- L1正则化相比L2正则化能使得模型参数具有稀疏解的原理？
 1. L1：稀疏，好处是特征选择、可解释性强；
 2. L2: 可以看成是权值的高斯先验；
    让求解变得稳定；
    数值上更容易求解；
    控制模型的复杂度，光滑性；复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。
    减小参数空间；参数空间越小，复杂度越低。 系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。
 3. 对比：3个角度：
      依据KKT条件可得带约束条件和带正则等价，观察解空间和等高线；
      拉普拉斯先验和贝叶斯先验；
      函数叠加

- 训练数据量很大时，经典的梯度下降法存在什么问题？需要如何改进？
 1. 随机梯度下降

- 牛顿法和拟牛顿法的理解？

- 随机梯度下降法的变种？
  - SGD，学习过程可能会比较慢
  - Momentum，动量方法，加速学习
  - AdaGrad，AdaDelta，自适应学习率 
  - RMSProp，
  - Adam，AdaMax, Nadam
  - Ftrl，Nesterov 

- 启发式搜索算法有哪些？避免局部最优？

- 拉格朗日乘子法：KKT条件？解的下界？对偶问题？

- 如何判断一个问题是不是凸优化问题，举例？
 1. 凸优化（逻辑回归）、非凸优化（主成分分析、神经网络）

## 采样

- 数据不平衡问题对模型有什么影响？数据偏斜不能过于严重（unbalanced），不同类别的数据数量不要有数个数量级的差距，数据不平衡问题如何解决？
  1. 影响：
  2. 解决：
       阈值移动，
       上采样(smote，有放回)，
       下采样（EasyEnsemble），
       代价敏感学习（修改权重）

- 如果数据量太大怎么处理？

## 概率\统计\信息论等

- 理解独立和相关？频率学派和贝叶斯学派？大数定律？贝叶斯公式，协方差矩阵？

- 极大似然思想？参数估计的置信区间、置信度的含义？假设检验的思想？
 1. 目的：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
    原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。
    极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

- 互信息，交叉熵，条件熵，K-L散度？

- 协方差和相关性有什么区别？
  相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

- 采样多少合适？论样本与总体、置信度的关系。

- 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？答：约有32%的数据将不受缺失值的影响。因为，由于数据分布在中位数附近，让我们先假设这是一个正态分布。我们知道，在一个正态分布中，约有68%的数据位于跟平均数（或众数、中位数）1个标准差范围内，那么剩下的约32%的数据是不受影响的。因此，约有32%的数据将不受缺失值的影响。






