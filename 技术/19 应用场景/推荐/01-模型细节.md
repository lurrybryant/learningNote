传统推荐：
1）协同过滤算法族：
  cf(usercf, itemcf 2003 Amazon) 1992，Xerot研究中心
    usercf：共现矩阵，用户相似度（余弦、皮尔逊系数），加权平均，
      缺陷：1. 用户相似度矩阵存储开销大；2. 用户行为数据稀疏，低频应用不适用；
      适用社交特性，新闻热点；
    itemcf: 利用物品相似度矩阵，计算与正反馈相似的物品累加相似度并排序
      适用兴趣变化稳定的应用；
    缺点：
      协同过滤不具备较强的泛化性；
      推头部物品，处理稀疏向量能力弱；
      仅利用交互信息，无法有效利用物品和用户特征；
  mf（加入矩阵分解，提升处理稀疏矩阵的能力 2006，netflix）
    奇异值分解：复杂度高；要求矩阵稠密；
    梯度下降：平方差损失，利用全局信息得到隐向量，有更强的泛化能力；消除用户和物品打分偏差；
    特点：泛化能力强，空间复杂度低，更好的扩展性和灵活性；
         丧失用户信息等，且在缺乏用户行为时无法进行有效推荐；
2）逻辑回归模型族：
  lr(加入更多特征)：数学含义的支持ctr；可解释性强；工程化强；表达能力不强，无法进行特征交叉
  ls-plm(样本分组+lr) 2012，阿里巴巴，也叫MLR 2017
    端到端非线性学习能力；模型具有稀疏性；
  poly2(特征交叉)：暴力组合；稀疏，导致很难收敛；参数数量从n变为n方；
3）因子分解机模型：
  fm(加入更多特征，通过向量特征交叉，解决poly2稀疏性和计算复杂性问题) 2010，Rendle
    隐权重向量，nk个参数；更好地解决数据稀疏问题；泛化能力提高；工程方便；
  ffm(引入特征域信息、多域隐向量, 2016) Criteo
    模型表达能力更强；复杂度也更高；
4）组合模型：
  gbdt+lr(gbdt进行特征组合和帅选) 2014 Facebook

DNN出发：
1）改变复杂度：
  AutoRec 2015，澳大利亚国立大学，
    自编码器重建函数+协同过滤推荐
    表达能力不足；
  Deep Crossing 2016 微软，
    网络结构：embedding，堆叠，残差网络（你和输出与输入的残差），得分；
2) 改变特征交叉方式：
  PNN 2016, 上海交通大学，加入多组特征向量交叉，相比Deep Crossing 增加了乘积层，内积和外积：多组特征交叉方式；
    缺点：无差别交叉，一定程度忽略了原始特征中有价值的信息；
  NeuralCF 2017 新加坡国立大学，互操作方式从内积改为神经网络，混合模型，可以灵活地进行互操作；
    缺点：没有其他类型特征，本质还是协同过滤；
3）组合模型：
  WDL 2016，谷歌 记忆（模型直接学习并利用数据中物品或者特征的共现频率的能力）和泛化（模型传递特征的相关性，发掘稀疏甚至从未出现过的特征与最终标签相关性的能力）
    特点：融合传统模型的记忆能力和深度模型的泛化能力；
  Deep&Cross, DCN 2017，斯坦福大学和谷歌研究人员，Cross替代wide，类似外积基础上添加权重，
  DeepFM 2017 华为和哈尔滨工业大学，利用FM替代Wide
4）FM衍化版：
  FNN 2016, 伦敦大学，结构类似Deep Crossing，Embedding（收敛慢的原因是参数量大而输入向量稀疏）初始化方式FM
  NeuralFM 2017 NFM 新加坡国立大学，用深度网络替代二阶部分，特征交叉池化，
  AFM，
5) 注意力机制：
  AFM 2017，浙江大学，特征交叉层，注意力网络提供注意力得分的池化层
  DIN 2017，阿里巴巴，注意力机制得分，历史商品id加权和，激活单元
  基于对用户行为的观察；
6）序列模型：
  DIEN 2019，阿里巴巴，兴趣衍化，兴趣进化网络（行为序列层，兴趣抽取层GRU，兴趣进化层AUGRU）
7）强化学习：
  DRN，2018，宾夕法尼亚州立大学和微软，框架、网络、学习过程、在线学习（竞争梯度下降）


