MMOE：
1. 介绍：
研究任务之间的差异关系和任务内部优化目标很重要
多任务学习利用正则化和模型迁移学习可以提升性能
MoE集成网络：PathNet、MoE layer
相关性越大，损失越小，效果越好；
2. 特点：
Single-Task：在辅助任务上效果最好；
Shared-Bottom：比其它偏门方法还差；效果次好，避免过拟合，但是任务差异性带来学习困难；
L2-Constrained：增加约束
Cross-Stitch：任务相关参数
Tensor-Factorization：任务相关参数；效果最差，当任务不相干时还考虑泛化性；需要更多时间和数据；
OMoE：和Single-Task比降低投入度和提升满意度；
MMoE：和OMoE比能提升投入度和满意度；
  满意度的系数更集中；
  任务不相干时更能发挥作用；
  训练非凸神经网络中，更容易训练；
  计算效率高；门网络topK系数化；
  学习到专家混合模式，捕捉到任务之间的关系；
  相关性越大，和其它模型差异越小；
  MOE结构比Shared-Bottom效果好且更好训练；


ESMM：
1. 介绍：
点击后cvr模型：数据稀疏、样本选择偏差；
稀疏问题：OVERSAMPLING过采样方法对采样率敏感；All Missing As Negative存在一致性低估；UNBIAS rejection sampling数值不稳定；
delayed feedback：
2. 特点：
ESMM：直接在全空间建模；损失函数：包含ctr损失和ctcvr损失，交叉熵；
  应用特征表示迁移学习策略，共享embedding层字典，减缓数据稀疏问题；
  利用序列行为信息；
  ESMM-NS 不共享embedding
  DIVISION 分别训练；


ESM2：
1. 介绍：
2. 特点：利用所有曝光样本和额外冗余监督信号，并行预测分解子目标：impression→click→D(O)Action→purchase；
Shared Embedding Module (SEM):
Decomposed Prediction Module (DPM)：
Sequential Composition Module (SCM)：
预测：CTCVR、CTAVR、CTR、CVR
SSB：所有损失从全空间下计算；
DS：子任务利用丰富的有标签数据；
GAUC
embedding：利用标准化方式进行


AITM：自适应信息迁移多任务
1. 介绍：
概率迁移模式：只能迁移概率信息，忽略了更多有用的表示；
MoSE：MOE系列；建模序列；
NMTR：ESMM系列；协同过滤；
迁移概率：损失向量信息、预测不准会受影响；
tensor normal priors \ attention 
NHFM DIFM
2. 特点：建模序列依赖，
  AIT自适应学习迁移不同程度转化的信息
  损失函数中结合行为期望校准
  类似self-attention



PLE：
1. 介绍：
由于复杂相关性，任务之间负面影响；
negative transfer：seesaw phenomenon
2. 特点：
把共享部分和任务特殊部分分开，采用progressive routing机制提取语义信息，提升联合学习和信息路由的效率。
Hard parameter sharing：
cross-stitch network：
sluice network：捕捉不了样本依赖；
MMoE：忽略专家之间的交互；
MRAN：多头自注意机制
CGC：避免复杂任务相关性问题；
AutoML：SNR
协同过滤和矩阵分解；
hard参数negative transfer and seesaw phenomenon
hierarchical multi-pointer co-attention










