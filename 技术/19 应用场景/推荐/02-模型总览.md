传统推荐：
1）协同过滤算法族：
根据用户行为历史生成用户-物品共现矩阵；
利用用户相似性和物品相似性进行推荐；
原理简单直接，应用广泛；
泛化能力差；
处理稀疏矩阵的能力差；
推荐结果的头部效应较明显；
  cf(usercf, itemcf 2003 Amazon) 1992，Xerot研究中心[2]
    content-based cf：基于内容的相似性推荐[1]
    usercf：共现矩阵，用户相似度（余弦、皮尔逊系数），加权平均，
      缺陷：1. 用户相似度矩阵存储开销大；2. 用户行为数据稀疏，低频应用不适用；
      适用社交特性，新闻热点；[3]
    itemcf: 利用物品相似度矩阵，计算与正反馈相似的物品累加相似度并排序
      适用兴趣变化稳定的应用；[4]
    缺点：
      协同过滤不具备较强的泛化性；
      推头部物品，处理稀疏向量能力弱；
      仅利用交互信息，无法有效利用物品和用户特征；
  als[5] 
  item-to-item based cf：利用物品相似度矩阵；
计算与正反馈相似的物品累加相似度；
排序；适用兴趣变化稳定的应用，商品、电视剧等场景；不具备较强的泛化性；
推头部物品，处理稀疏向量能力弱；
仅利用交互信息，无法有效利用物品和用户特征；[5]
  SlopeOne[6]
  mf（加入矩阵分解，提升处理稀疏矩阵的能力 2006，netflix）
    奇异值分解：复杂度高；要求矩阵稠密；
    梯度下降：平方差损失，利用全局信息得到隐向量，有更强的泛化能力；消除用户和物品打分偏差；
    特点：泛化能力强，空间复杂度低，更好的扩展性和灵活性；
         丧失用户信息等，且在缺乏用户行为时无法进行有效推荐；
  mf【user-based】将共现矩阵分解为用户矩阵和物品矩阵；
用户向量和物品向量内积排序；加入矩阵分解，提升处理稀疏矩阵的能力；复杂度高；要求矩阵稠密；
  mf-svd
  mf-svd++【item cf】
  svd feature[7]
  BPR-MF【user-based】为隐反馈数据做top-N推荐，利用pair-wise算法建模用户偏好[8]
  Funksvd：[9]
  梯度下降mf[10]：平方差损失，利用全局信息得到隐向量，有更强的泛化能力；
消除用户和物品打分偏差；泛化能力强，空间复杂度低，更好的扩展性和灵活性；丧失用户信息等，且在缺乏用户行为时无法进行有效推荐；
  SBPR[11] 将社交信息引入BPR框架
  PMF[12] 概率图模型
  SoRec[13] 社交信息构造概率图
  SoReg[14] 增加用户平滑项的方式来约束用户的隐表示应尽可能和社交邻居相似
  SLIM【item】[15]Top-N推荐，稀疏线性模型 易并行且可获得稀疏解
  FISM【item】[16]
  Trust-SVD[17]: 结合了评分信息和社交信息显示和隐式数据，社会化推荐
  RBCF[18] 限制玻尔兹曼机协同过滤. 第一次深度学习结合协同过滤
  TrustWalker [19] 结合了item-base cf和基于信任的方法

2）逻辑回归模型族：
  lr(加入更多特征)：数学含义的支持ctr；可解释性强；工程化强；表达能力不强，无法进行特征交叉
  ls-plm(样本分组+lr) 2012，阿里巴巴，也叫MLR 2017[20]
    端到端非线性学习能力；模型具有稀疏性；每个样本的分片概率与逻辑回归加权平均得到预估值；三层神经网络具备较强表达能力；模型结构比深度学习简单
  poly2(特征交叉)：暴力组合；稀疏，导致很难收敛；参数数量从n变为n方；

3）因子分解机模型：[21]
  fm(加入更多特征，通过向量特征交叉，解决poly2稀疏性和计算复杂性问题) 2010，Rendle
    隐权重向量，nk个参数；更好地解决数据稀疏问题；泛化能力提高；工程方便；组合爆炸，不能扩展到三阶特征交叉阶段；在逻辑回归基础上，加入二阶交叉部分，为每一维特征训练隐向量，通过内积得到交叉特征权重；
隐权重向量，nk个参数；
  ffm(引入特征域信息、多域隐向量, 2016[22]) Criteo
    模型表达能力更强；复杂度也更高；训练开销n^2；引入特征域信息、多域隐向量；每个特征与不同特征域交叉时采用不同的隐向量；

4）组合模型：
  gbdt+lr(gbdt进行特征组合和帅选) 2014 [23]Facebook；GBDT不能并行训练，更新需要的周期长；特征工程模型化，使得模型具备了更高阶特征组合的能力；

5) 概率图模型：



DNN出发：
1）改变复杂度：
  AutoRec 2015，澳大利亚国立大学，[24]
    自编码器重建函数+协同过滤推荐
    表达能力不足；对用户和物品编码，利用自编码器的泛化能力进行推荐；单隐层神经网络快速训练和部署；
  Deep Crossing 2016 微软，[25]
    网络结构：embedding，堆叠，残差网络（拟合输出与输入的残差），得分；预完成特征的自动交叉；经典深度学习推荐系统框架；全连接隐层特征交叉，针对性不强

2) 改变特征交叉方式：
  PNN 2016[26], 上海交通大学，加入多组特征向量交叉，相比Deep Crossing 增加了乘积层，内积和外积：多组特征交叉方式；
    缺点：无差别交叉，一定程度忽略了原始特征中有价值的信息；在经典框架基础上提高特征交叉能力；
  NeuralCF[27] 2017 新加坡国立大学，互操作方式从内积改为神经网络，混合模型，可以灵活地进行互操作；表达能力加强的矩阵分解模型
    缺点：没有其他类型特征，本质还是协同过滤；
  YoutubeDNN[28]：
  ConvMF[29]：
  CCPM[30]：
  CrossNet：
  TEM[31]：

3）组合模型：
  WDL 2016[32]，谷歌 记忆（模型直接学习并利用数据中物品或者特征的共现频率的能力）和泛化（模型传递特征的相关性，发掘稀疏甚至从未出现过的特征与最终标签相关性的能力）
    特点：融合传统模型的记忆能力和深度模型的泛化能力；开创组合模型的构造方法；Wide部分需要人工进行特征组合；
  Deep&Cross, DCN [33]2017，斯坦福大学和谷歌研究人员，Cross替代wide，类似外积基础上添加权重，解决wide部分人工组合特征问题；Cross部分负责度高  
  DCN V2[34]
  DeepFM [35] 2017 华为和哈尔滨工业大学，利用FM替代Wide，加强wide部分特征交叉能力，与wide and deep结构差别不明显
  xDeepFM [36]
  DLRM [37]
  DeepGBM [38]

4）FM衍化版：
  FNN 2016 [39], 伦敦大学，结构类似Deep Crossing，Embedding（收敛慢的原因是参数量大而输入向量稀疏）初始化方式FM；利用FM初始化参数，加快收敛速度；模型结构简单，没有针对性的特征交叉层
  NeuralFM 2017 [40] NFM 新加坡国立大学，用深度网络替代二阶部分，特征交叉池化，相比FM的特征交叉和表达能力更强；与PNN的结构很类似
  FGCNN，[41]
  FiBiNET，[42]

5) 注意力机制：
  AFM[43] 2017，浙江大学，特征交叉层，注意力网络提供注意力得分的池化层；在FM的基础上，在二阶隐向量交叉的基础上对每个交叉结果加入注意力得分，注意力网络提供注意力得分的池化层；不同交叉特征的重要性不同；注意力网络训练复杂
  DIN[44] 2017，阿里巴巴，注意力机制得分，基于对用户行为的观察历史商品id加权和，激活单元；根据目标广告物品的不同进行更有针对性的推荐；没有充分利用除历史行为以外的其它特征；
  AutoInt[45]
  

6）序列模型：
  DIEN[46] 2019，阿里巴巴，兴趣衍化，兴趣进化网络（行为序列层，兴趣抽取层GRU，兴趣进化层AUGRU）；训练模型训练复杂，线上延时较长；序列模型增强了系统对用户兴趣变迁的表达能力，使得推荐系统开始考虑时间相关的行为序列中包含的有价值的信息；兴趣衍化，兴趣进化网络（行为序列层，兴趣抽取层GRU，兴趣进化层AUGRU）；将序列模型与深度模型结合，使用序列模型模拟用户的兴趣进化过程；
  SASRec  2018
  BERT4Rec  2019
  Caser[74]
  GRU4Rec[47] 

  SIM 2020
  DMT 2020
  ComiRec 2020
  DSIN  2019
  MIMN  2019
  DSTN  2019
  MIND  2019
  BST 2019

7）强化学习：
  DRN[48]，2018，宾夕法尼亚州立大学和微软，框架、网络、学习过程、在线学习（竞争梯度下降）;将强化学习的思想应用到推荐，线上实时学习更新;模型对实时数据利用能力大 大加强;线上复杂，工程实现较难

8) 双塔，多任务：
  ESMM[51]：
  ESM2
  MoE
  MMOE[52]：youtube
  PLE[50]：
  AITM：美团；
  DUPN
  SNR
  Share Bottom [81]
  GemNN：
  DSSM[49]：User和Item两个网络

9）跨域任务学习
  Multi-Domain：快手\阿里


9）预训练模型
  U-BERT[53] 行为数据的稀疏使我们难以学习到精确的用户表示，为了解决这个问题，一个想法是:利用内容丰富的域来补充用户表示。
在预训练阶段，U-BERT专注于内容丰富的领域，并引入了一个用户编码器和一个评论编码器来模拟用户的行为。提出了两种预训练策略来学习用户的一般表征；
在微调阶段，U-BERT主要关注目标内容不足的域。除了从预训练阶段继承的用户和review编码器之外，U-BERT还引入了一个商品编码器来对商品表示进行建模。此外，还提出了一个评论匹配层来捕获用户评论和项目评论之间更多的语义交互。
最后，U-BERT结合用户表示、项目表示和交互信息来提高推荐性能。

10）排序模型
  DGR [54] 

11）协同过滤
  DeepICF[55]
  NAIS[56]
  ACF[57]
  DeepMF [58] 
  ONCF[59]
  NeuMF[60]

12）表示学习
  DeepCF[61]

13）图神经网络
  PinSage 
  Pinterest 
  GCN[62]
  NGCF[63]
  SRGNN[64]
  SGL[65]

14）知识图谱
  RippleNet[66]

15）embedding
  item2vec[67]
  雅虎embedding[68]
  word2vec 2013， 
  graph embedding deep walk 2014, 
  node2vec 2016 
  airbnb embedding 2018
  EGES（2018，淘宝, 加入补充信息，加权平均池化），
  LINE, 
  SDNE

20）其它模型
  FLEN [69]
  onn[70]
  SDM[71]
  CML[72]
  DeepCoNN[73]
  AoAFFM[75]
  AutoFIS[76]
  dqn 深度Q网络[77]
  NFFM[78]
  TFNET[79]
  FwFM[80]
  pid 流量调控


16）召回模型：
  TDM





# 面试


# 论文
81. One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction
80. [WWW 2018]Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising
79. Wu S, Yu F, Yu X, et al. TFNet: Multi-Semantic Feature Interaction for CTR Prediction[C]//Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020: 1885-1888.
78. Neural Factorization Machines for Sparse Predictive Analytics
77. Sorokin I, Seleznev A, Pavlov M, et al. Deep attention recurrent Q-network[J]. arXiv preprint arXiv:1512.01693, 2015.
76. Liu B, Zhu C, Li G, et al. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020: 2636-2645.
75. Wang, Z., Ma, J., Zhang, Y., Wang, Q., Ren, J., & Sun, P. (2020). Attention-over-Attention Field-Aware Factorization Machine. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04), 6323-6330.
74. Chen et al. Sequential Recommendation with User Memory Networks. WSDM 2018.
73. Lei et al. Joint Deep Modeling of Users and Items Using Reviews for Recommendation. WSDM, 2017.
72. Hsieh et al. Collaborative metric learning. WWW, 2017.
71. SDM: Sequential Deep Matching Model for Online Large-scale Recommender System
70. Operation-aware Neural Network for User Response Prediction：https://arxiv.org/abs/1904.12579 2019
69. [arxiv 2019]FLEN: Leveraging Field for Scalable CTR Prediction
68. ShumpeiOkura,YukihiroTagami,ShingoOno,andAkiraTajima.2017.Embedding-basedNewsRecommendationforMillionsof Users. In SIGKDD.
67. Barkan et al. Item2vec: neural item embedding for collaborative filtering. 2016.
66. Hongwei et al. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. CIKM, 2018.
65. 王翔、何向南, Self-supervised Graph Learning for Recommendation, SIGIR, 2021
64. Wu et al. Session-based Recommendation with Graph Neural Networks. AAAI, 2019.
63. Xiang et al. Neural Graph Collaborative Filtering. SIGIR, 2019.
62. Ying et al. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. KDD, 2018.
61. Zhi-Hong et al. DeepCF: A Unified Framework of Representation Learning and Matching Function Learning in Recommender System. AAAI, 2019.
60. Chen C, Zhang M, Zhang Y, et al. Efficient neural matrix factorization without sampling for recommendation[J]. ACM Transactions on Information Systems (TOIS), 2020, 38(2): 1-28.
59. Outer Product-based Neural Collaborative Filtering
58. Lara-Cabrera R, González-Prieto Á, Ortega F. Deep matrix factorization approach for collaborative filtering recommender systems[J]. Applied Sciences, 2020, 10(14): 4926.
57. Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention 2017
56. He X, He Z, Song J, et al. Nais: Neural attentive item similarity model for recommendation[J]. IEEE Transactions on Knowledge and Data Engineering, 2018, 30(12): 2354-2366.
55. Xue F, He X, Wang X, et al. Deep item-based collaborative filtering for top-n recommendation[J]. ACM Transactions on Information Systems (TOIS), 2019, 37(3): 1-25.
54. Huafeng et al. Deep Generative Ranking for Personalized Recommendation. Recsys, 2019.
53. U-BERT: Pre-training User Representations for Improved Recommendation, AAAI, 2021
52. Ma J, Zhao Z, Yi X, et al. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2018: 1930-1939.
51. Ma X, Zhao L, Huang G, et al. Entire space multi-task model: An effective approach for estimating post-click conversion rate[C]//The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 2018: 1137-1140.

1. Balabanovic et al. Fab: Content-based, collaborative recommendation.1997.
2. Xerot研究中心, Goldberg et al. Using collaborative filtering to weave an information tapestry. Communications of the ACM, 1992. 
3. User-based CF of Netnews GroupLens: An Open Architecture for Collaborative Filtering of Netnews. 1994.
4. Sarwar et al. Item-based collaborative filtering recommendation algorithms. WWW, 2001.
5. Zhou Y, Wilkinson D, Schreiber R, et al. Large-scale parallel collaborative filtering for the netflix prize[C]//International conference on algorithmic applications in management. Springer, Berlin, Heidelberg, 2008: 337-348.
5. Linden et al. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE INTERNET COMPUT, 2003.
6. Lemire et al. Slope One Predictors for Online Rating-Based Collaborative Filtering, SDM, 2005 .
7. Daniel et al. Learning Collaborative Information Filters. ICML,1998.
Computation of the singular value decomposition, 2006
Koren et al. Matrix factorization techniques for recommender systems. Computer, 2009.
8. Rendle et al. BPR: Bayesian personalized ranking from implicit feedback. UAI, 2009.
9. Simon Funk. Netflix Update: Try This at Home:https://sifter.org/~simon/journal/20061211.html, 2006.
10. Bell et al. The BellKor solution to the Netflix Prize.2007.
11. Zhao et al. Leveraging social connections to improve personalized ranking for collaborative filtering. CIKM, 2014.
12. Mnih et al. Probabilistic matrix factorization. NIPS, 2008.
13. Ma et al. Sorec: social recommendation using probabilistic matrix factorization. CIKM, 2008.
14. Ma et al. Recommender systems with social regularization. WSDM, 2011.
15. Ning et al. SLIM: Sparse linear methods for top-n recommender systems. ICDM, 2011.
16. Kabbur S, Ning X, Karypis G. Fism: factored item similarity models for top-n recommender systems[C]//Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. 2013: 659-667.
17. Guo et al. TrustSVD: Collaborative Filtering with Both the Explicit and Implicit Influence of User Trust and of Item Ratings. AAAI, 2015.
18. Ruslan et al. Restricted Boltzmann Machines for Collaborative Filtering,ICML, 2007.
19. Jamali et al. Trustwalker: a random walk model for combining trust-based and item-based recommendation. KDD, 2009.
20. 阿里巴巴 Gai et al. Learning Piece-wise Linear Models from Large Scale Data for Ad ClickPrediction, 2017.
21. Rendle. Factorization machines. ICDM, 2010.
22. Yuchin et al. Field-aware Factorization Machines for CTR Prediction. RecSys,
23. He et al. Practical lessons from predicting clicks on ads at facebook. 2014.
24. 澳大利亚国立大学, Sedhain et al. Autorec: Autoencoders meet collaborative filtering. WWW, 2015.
25. 微软 Shan et al. Deep crossing: Web-scale modeling without manually crafted combinatorial features. KDD, 2016.
26. 上海交通大学 Yanru et al. Product-based Neural Networks for User Response Prediction. ICDM, 2016.
27. 新加坡国立大学 He et al. Neural collaborative filtering. WWW, 2017.
28. Covington et al. Deep Neural Networks for YouTube Recommendations. 2016.
29. Kim et al. Convolutional Matrix Factorization for Document Context-Aware Recommendation. RecSys, 2016
30. Liu Q, Yu F, Wu S, et al. A convolutional click prediction model[C]//Proceedings of the 24th ACM international on conference on information and knowledge management. 2015: 1743-1746.
31. Wang X, He X, Feng F, et al. Tem: Tree-enhanced embedding model for explainable recommendation[C]//Proceedings of the 2018 World Wide Web Conference. 2018: 1543-1552.
32. 谷歌 Cheng et al. Wide & Deep Learning for Recommender Systems. 2016.
33. 斯坦福大学和谷歌研究人员 Ruoxi et al. Deep & Cross Network for Ad Click Predictions. ADKDD, 2017.
34. Wang R, Shivanna R, Cheng D Z, et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems[J]. 2020.
35. 华为和哈尔滨工业大学 Guo et al. Deepfm: A factorization-machine based neural network for ctr prediction. IJCAI, 2017.
36. Lian et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. KDD, 2018.
37. Deep Learning Recommendation Model for Personalization and Recommendation Systems 2019
38. Ke G, Xu Z, Zhang J, et al. DeepGBM: A deep learning framework distilled by GBDT for online prediction tasks[C]//Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019: 384-394.
39. 伦敦大学 Zhang et al. Deep Learning over Multi-field Categorical Data. UCL, 2016.
40. 新加坡国立大学 He et al. Neural Factorization Machines for Sparse Predictive Analytics. 2017.
41. Liu B, Tang R, Chen Y, et al. Feature generation by convolutional neural network for click-through rate prediction[C]//The World Wide Web Conference. 2019: 1119-1129.
42. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction,recsys, 2019
43. 浙江大学 Xiao et al. Attentional factorization machines: Learning the weight of feature interactions via attention networks. IJCAI, 2017.
44. 阿里巴巴 Zhou et al. Deep interest network for click-through rate prediction. KDD, 2018.
45. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks(CIKM, 2019)
46. 阿里巴巴 Zhou et al. Deep interest evolution network for click-through rate prediction. AAAI, 2019.
47. Hidasi et al. Session-based Recommendations with Recurrent Neural Networks. 2016.
48. 宾夕法尼亚州立大学和微软 DRN: A Deep Reinforcement Learning Framework for News Recommendation www 2018
49. 微软 Huang et al. Learning deep structured semantic models for web search using clickthrough data. 2013.
50. Tang H, Liu J, Zhao M, et al. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations[C]//Fourteenth ACM Conference on Recommender Systems. 2020: 269-278.











