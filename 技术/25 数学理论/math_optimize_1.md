## 凸优化

- 凸集 凸函数 凸优化
-    /alpha 水平集

## 数值优化
-  方法
  -  坐标下降：循环使用不同的坐标方向来达到目标函数的局部最小值；
  -  梯度下降：平面逼近局部
也叫最速下降法；
负梯度下降  http://blog.csdn.net/u012328159/article/details/51613262 
深度下降 
批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。 
随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。
  -  随机梯度下降SGD：增量梯度下降
通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
 
随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量 
  -  批梯度下降BGD：迭代速度会相当的慢
  -  mini batch： 
  -  牛顿法：曲面逼近局部；
迭代轮数远小于梯度下降；
牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部
优点：二阶收敛，收敛速度快；
缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
  -  拟牛顿法：寻找嗨森矩阵的近似逆矩阵；
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度
常用的拟牛顿法有DFP算法和BFGS算法，L-BFGS
  -  近端梯度下降PGD
  -  共轭梯度CG：共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数
共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点
  -  启发式搜索算法：模拟退火方法、遗传算法、蚁群算法以及粒子群算法
  -  多目标优化算法：NSGAII算法、MOEA/D算法以及人工免疫算法
  -   拉格朗日乘子法：KKT条件，解的下界：对偶问题

## 信息论
 熵 联合熵 条件熵 互信息 交叉熵 KL距离（相对熵）信息增益 信息增益率
  基尼指数